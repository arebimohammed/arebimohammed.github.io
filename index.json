[{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"An open-source exploration of the city's taxi life, boroughs, neighborhoods, and more via the prism of publicly available taxi data","date":"2020-10-16","objectID":"/new-york-taxi-analysis/","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":" This article goes in detail through one of the data science projects I worked on, the New York Taxi dataset which is made available by the New York City Taxi and Limousine Commission (TLC). New York is partly known for its yellow taxis (even though there are green taxis we‚Äôll focus on only the yellow taxis) and there are millions of taxi rides taken every month as New York is one of the most populous cities in the United States which makes for a very interesting dataset to dive into as there are a lot of analysis possibilities (Geospatial Data Analysis, Time Analysis, Taxi Ride Price Analysis, Traffic Analysis, etc.). ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:0:0","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Introduction When seen as a whole, the precise trip-level data is more than simply a long list of taxi pickup and drop-off locations: it tells a tale of New York. How much do taxis make each trip if they can only travel 30 or 50 kilometres per trip, as opposed to taxis with no limit? Is it more common for taxi journeys to begin in the north or south? What is the average speed of New York traffic (with regards to taxis)? How does the speed of traffic in New York change during the day? All of these questions, and many more, are addressed in the dataset. And we will answer them as well as others. ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:1:0","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"The Process The usual data science process starts with business and feasability study, data collection, data cleaning, exploratory data analysis (EDA) and sometimes modelling. Of course there is more to this simple process abstraction, such as model deployment and monitoring but in our case these are skipped as we will focus on the data collection, cleaning and exploratory analysis. We‚Äôll now go through the different phases one by one. ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:2:0","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Data Collection The data is taken from this link: NYC Yellow Taxi provided by Andr√©s Monroy. The link contains data contains compressed files of yellow taxi rides for the year 2013. There are two compressed files one named trip_data is for trip specific data such as the distance, time, etc while the other one named trip_fare contains data regarding the trip fare (hence the name), surchage, tip, etc. Both files divide the data in 12 CSV files (one for each month). The data is also available in Kaggle for the trip data and trip fare here and here respectively. Kaggle wasn‚Äôt used as it doesn‚Äôt contain all the data (even though we are also not using all of it, more below) and for the fare data the data is suffled between several years. The data dictionary/description can be found here: Data Dictionary, note that not all columns are in the data dictionary from the data source somehow but looking further in the analysis they are all made clear and are mostly intuituve. After downloading the compressed files and unzipping them they are about 5.4 GB in total, which is quite hefty for my local machine as I am doing the analysis locally on a small machine and not on the cloud. This is where data sampling comes into play. Reservoir random sampling was used to sample 150K records/trips per month of 2013. The code for this is shown here and is also available in the projects github repo. import pandas as pd import numpy as np import os from itertools import islice from io import StringIO from tqdm.notebook import tqdm pd.set_option(\"display.max_columns\",None) np.random.seed(42) def reservoir_sample(iterable, random1, random2,random3,random4, k=1): ''' From : https://en.wikipedia.org/wiki/Reservoir_sampling#An_optimal_algorithm ''' iterator = iter(iterable) values = list(islice(iterator, k)) W = np.exp(np.log(random1/k)) while True: skip = int(np.floor(np.log(random2)/np.log(1-W))) selection = list(islice(iterator, skip, skip+1)) if selection: values[random3] = selection[0] W *= np.exp(np.log(random4)/k) else: return values def df_sample(filepath1,filepath2, k): r1,r2,r3,r4 = np.random.random(), np.random.random() ,np.random.randint(k), np.random.random() with open(filepath1, 'r') as f1, open(filepath2, 'r') as f2: header1 = next(f1) header2 = next(f2) values1 = reservoir_sample(f1,r1,r2,r3,r4,k) values2 = reservoir_sample(f2,r1,r2,r3,r4,k) result1 = [header1] + values1 result2 = [header2] + values2 df1 = pd.read_csv(StringIO(''.join(result1))) df2 = pd.read_csv(StringIO(''.join(result2))) df1 = sample_preprocessing(df1) df2 = sample_preprocessing(df2, fare= True) #f2 has to be fare data return df1,df2 def sample_preprocessing(df_sample, fare= False): df_sample.columns = df_sample.columns.str.replace(\" \",\"\") if fare: df_sample.drop(columns=[\"hack_license\",\"vendor_id\"],inplace= True) return df_sample def reduce_memory(df, verbose=True): numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"] start_mem = df.memory_usage().sum() / 1024 ** 2 for col in df.columns: col_type = df[col].dtypes if col_type in numerics: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == \"int\": if c_min \u003e np.iinfo(np.int8).min and c_max \u003c np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min \u003e np.iinfo(np.int16).min and c_max \u003c np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min \u003e np.iinfo(np.int32).min and c_max \u003c np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min \u003e np.iinfo(np.int64).min and c_max \u003c np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if ( c_min \u003e np.finfo(np.float16).min and c_max \u003c np.finfo(np.float16).max ): df[col] = df[col].astype(np.float16) elif ( c_min \u003e np.finfo(np.float32).min and c_max \u003c np.finfo(np.float32).max ): df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) end_mem = df.memory_usage().sum() / 1024 ** 2 if verbose: print( \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format( end_mem, 100 * (s","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:3:0","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Data Cleaning In this phase of the project we‚Äôll check the data for any inconsistencies, outliers, erroneous values, etc. The cleaning itself is divided into several parts such as location data (pickup, dropoff) cleaning, trip duration cleaning, trip distance cleaning, trip speed cleaning, trip total amount, fare Amount, surcharge, tax, tolls and tip cleaning and categorical columns cleaning. We‚Äôll also use this phase to calculate and add some other features/columns as well. Let‚Äôs get straight to it üòç! ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:0","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Import Libraries and Data Loading import warnings warnings.filterwarnings(\"ignore\") import pandas as pd import numpy as np import matplotlib.pyplot as plt import datetime import requests import json import os from tqdm.notebook import tqdm import geopy.distance from shapely.geometry import Point import folium import geopandas as gpd from geopandas import GeoDataFrame import geoplot import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots pd.set_option(\"display.max_columns\",None) np.random.seed(42) df = pd.read_csv(\"../Data/Pre-processed Data/data.csv\") df.head() Unnamed: 0 medallion hack_license vendor_id rate_code store_and_fwd_flag pickup_datetime dropoff_datetime passenger_count trip_time_in_secs trip_distance pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude payment_type fare_amount surcharge mta_tax tip_amount tolls_amount total_amount 0 0 89D227B655E5C82AECF13C3F540D4CF4 BA96DE419E711691B9445D6A6307C170 CMT 1 N 2013-01-01 15:11:48 2013-01-01 15:18:10 4 382 1.0 -73.978165 40.757977 -73.989838 40.751171 CSH 6.5 0.0 0.5 0.0 0.0 7.0 1 1 0BD7C8F5BA12B88E0B67BED28BEA73D8 9FD8F69F0804BDB5549F40E9DA1BE472 CMT 1 N 2013-01-06 00:18:35 2013-01-06 00:22:54 1 259 1.5 -74.006683 40.731781 -73.994499 40.750660 CSH 6.0 0.5 0.5 0.0 0.0 7.0 2 2 0BD7C8F5BA12B88E0B67BED28BEA73D8 9FD8F69F0804BDB5549F40E9DA1BE472 CMT 1 N 2013-01-05 18:49:41 2013-01-05 18:54:23 1 282 1.1 -74.004707 40.737770 -74.009834 40.726002 CSH 5.5 1.0 0.5 0.0 0.0 7.0 3 3 DFD2202EE08F7A8DC9A57B02ACB81FE2 51EE87E3205C985EF8431D850C786310 CMT 1 N 2013-01-07 23:54:15 2013-01-07 23:58:20 2 244 0.7 -73.974602 40.759945 -73.984734 40.759388 CSH 5.0 0.5 0.5 0.0 0.0 6.0 4 4 DFD2202EE08F7A8DC9A57B02ACB81FE2 51EE87E3205C985EF8431D850C786310 CMT 1 N 2013-01-07 23:25:03 2013-01-07 23:34:24 1 560 2.1 -73.976250 40.748528 -74.002586 40.747868 CSH 9.5 0.5 0.5 0.0 0.0 10.5 df.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 1799998 entries, 0 to 1799997 Data columns (total 22 columns): # Column Dtype --- ------ ----- 0 Unnamed: 0 int64 1 medallion object 2 hack_license object 3 vendor_id object 4 rate_code int64 5 store_and_fwd_flag object 6 pickup_datetime object 7 dropoff_datetime object 8 passenger_count int64 9 trip_time_in_secs int64 10 trip_distance float64 11 pickup_longitude float64 12 pickup_latitude float64 13 dropoff_longitude float64 14 dropoff_latitude float64 15 payment_type object 16 fare_amount float64 17 surcharge float64 18 mta_tax float64 19 tip_amount float64 20 tolls_amount float64 21 total_amount float64 dtypes: float64(11), int64(4), object(7) memory usage: 302.1+ MB df.describe() Unnamed: 0 rate_code passenger_count trip_time_in_secs trip_distance pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude fare_amount surcharge mta_tax tip_amount tolls_amount total_amount count 1.799998e+06 1.799998e+06 1.799998e+06 1.799998e+06 1.799998e+06 1.799998e+06 1.799998e+06 1.799930e+06 1.799930e+06 1.799998e+06 1.799998e+06 1.799998e+06 1.799998e+06 1.799998e+06 1.799998e+06 mean 8.999985e+05 1.035788e+00 2.020107e+00 7.653680e+02 6.113157e+00 -7.286966e+01 4.013907e+01 -7.280162e+01 4.009319e+01 1.243297e+01 2.739124e-01 4.982975e-01 1.338854e+00 2.654285e-01 1.480971e+01 std 5.196148e+05 2.800236e-01 1.660724e+00 1.153058e+04 3.744392e+03 9.001916e+00 6.591034e+00 9.397887e+00 7.702412e+00 4.701487e+01 3.383309e-01 2.912650e-02 2.238275e+00 1.239370e+00 4.750405e+01 min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 -9.885000e+01 -3.114279e+03 -1.145424e+03 -3.113789e+03 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 25% 4.499992e+05 1.000000e+00 1.000000e+00 3.600000e+02 1.080000e+00 -7.399219e+01 4.073427e+01 -7.399150e+01 4.073344e+01 6.500000e+00 0.000000e+00 5.000000e-01 0.000000e+00 0.000000e+00 8.000000e+00 50% 8.999985e+05 1.000000e+00 1.000000e+00 6.000000e+02 1.820000e+00 -7.398193e+01 4.075231e+01 -7.398040e+01 4.075279e+01 9.500","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:1","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Convert dates to datetime type df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"]) df[\"dropoff_datetime\"] = pd.to_datetime(df[\"dropoff_datetime\"]) df.drop(df.columns[0], axis =1, inplace = True) ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:2","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Location Data Cleaning print(min(df.pickup_longitude.min(), df.dropoff_longitude.min()), \\ max(df.pickup_longitude.max(), df.dropoff_longitude.max())) print(min(df.pickup_latitude.min(), df.dropoff_latitude.min()), \\ max(df.pickup_latitude.max(), df.dropoff_latitude.max())) -1145.4238 1428.7383 -3114.2788 2234.9895 We see some very unexpected and erroneous latitude and longitude (location) data, let‚Äôs clean that up. Trips only within New York City NYBB = (-74.4505, -72.8478, 40.2734, 41.256) # https://boundingbox.klokantech.com/ before = df.shape[0] print(f'Rows/Size Before: {before}') df = df[(df.pickup_longitude \u003e= NYBB[0]) \u0026 (df.pickup_longitude \u003c= NYBB[1]) \u0026 \\ (df.pickup_latitude \u003e= NYBB[2]) \u0026 (df.pickup_latitude \u003c= NYBB[3]) \u0026 \\ (df.dropoff_longitude \u003e= NYBB[0]) \u0026 (df.dropoff_longitude \u003c= NYBB[1]) \u0026 \\ (df.dropoff_latitude \u003e= NYBB[2]) \u0026 (df.dropoff_latitude \u003c= NYBB[3])].reset_index(drop=True) after = df.shape[0] print(f'Rows/Size After: {after}') print(f'Removed: {before - after}') Rows/Size Before: 1799998 Rows/Size After: 1770605 Removed: 29393 We remove 29393 incorrect location records. ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:3","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Trip Dates and Duration Cleaning (df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]).describe() count 1770605 mean 0 days 00:12:17.623533763 std 0 days 00:11:57.343927277 min 0 days 00:00:00 25% 0 days 00:06:00 50% 0 days 00:10:00 75% 0 days 00:16:00 max 7 days 01:13:13 dtype: object We‚Äôre seeing weird trip durations of 7 days and 0 seconds here. before = df.shape[0] print(f'Rows/Size Before: {before}') invalid_trip_time_idx = np.where((df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]).dt.total_seconds().astype(np.int64) != df[\"trip_time_in_secs\"])[0] trip_time_in_secs_ser = (df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]).dt.total_seconds().astype(np.int64) df.iloc[invalid_trip_time_idx, df.columns.tolist().index(\"trip_time_in_secs\")] = trip_time_in_secs_ser.iloc[invalid_trip_time_idx] invalid_duration_idx = np.where(((df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]) == datetime.timedelta()) | ((df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]) \u003c datetime.timedelta()))[0] df.drop(invalid_duration_idx, inplace=True) df.reset_index(drop=True, inplace=True) df.insert(loc=df.columns.tolist().index(\"trip_time_in_secs\")+1,column=\"trip_time_in_min\",value = (df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]).astype(\"timedelta64[m]\")) after = df.shape[0] print(f'Rows/Size After: {after}') print(f'Removed: {before - after}') Rows/Size Before: 1770605 Rows/Size After: 1768663 Removed: 1942 Here we correct the invalid trip duration by calculating it from the pickup and dropoff datetime as well as remove the negative and trips with zero seconds. We do that using numpy‚Äôs np.where to get all the rows where the duration is invalid and correct that in line 6. We finally add the trip duration in minutes as well. (df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]).describe() count 1768663 mean 0 days 00:12:18.433447751 std 0 days 00:11:57.320887829 min 0 days 00:00:01 25% 0 days 00:06:00 50% 0 days 00:10:00 75% 0 days 00:16:00 max 7 days 01:13:13 dtype: object We corrected the incorrect trip duration, removed the zero and negative duration trips but we still see very low trip durations of 1 second and very high trip duration of 7 days. We can inspect the percentiles of the trip duration distribution to get an idea where to cut the maximum and minimum values. trip_duration_percentiles = df[\"trip_time_in_secs\"].quantile(np.round(np.arange(0.00, 1.01, 0.01), 2)) trip_duration_percentiles.loc[0.00:0.1] 0.00 1.0 0.01 107.0 0.02 120.0 0.03 120.0 0.04 180.0 0.05 180.0 0.06 180.0 0.07 180.0 0.08 212.0 0.09 240.0 0.10 240.0 Name: trip_time_in_secs, dtype: float64 trip_duration_percentiles.loc[0.90:] 0.90 1380.0 0.91 1440.0 0.92 1500.0 0.93 1560.0 0.94 1680.0 0.95 1740.0 0.96 1860.0 0.97 2040.0 0.98 2280.0 0.99 2700.0 1.00 609193.0 Name: trip_time_in_secs, dtype: float64 df_short_duration.trip_distance.describe() count 337.000000 mean 0.763501 std 3.118799 min 0.000000 25% 0.000000 50% 0.000000 75% 0.000000 max 31.700000 Name: trip_distance, dtype: float64 fig_short = px.scatter(df_short_duration, x=\"pickup_longitude\",y=\"pickup_latitude\") fig_short = fig_short.update_traces(marker=dict(size= 1),name=\"Short Duration Pickups\") fig_short.data[0].showlegend = True fig_short = fig_short.add_trace(go.Scatter(x=df_short_duration[\"dropoff_longitude\"], y=df_short_duration[\"dropoff_latitude\"], mode='markers', marker=dict(color=\"red\", size=1), name=\"Short Duration Dropoffs\", opacity=0.5)) fig_short = fig_short.update_layout(xaxis_title =\"Longitude\", yaxis_title = \"Latitude\") fig_short.show() We‚Äôre seeing the outline of New York so these weird durations might just be problems with the meter, we‚Äôll drop them. df_large_duration = df[(df.trip_time_in_secs \u003e= 43200)] # 12 hours max, See : https://www1.nyc.gov/assets/tlc/downloads/pdf/rule_book_current_chapter_58.pdf#page=34 df_large_duration.trip_distance.describe() count 3.000000 mean 96236.666667 std 166681.513487 min 0.200000 25% 3.050000 50% 5.900000 75% 144354.900000 max 288703.900000 Name: trip_distance, dtype: float64 before =","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:4","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Trip Distance Cleaning We follow the same percentile inspection procedure along with the describe function to check for outliers and invalid records. df.trip_distance.describe().apply(lambda x: '%.5f' % x) count 1768323.00000 mean 6.02009 std 3771.53508 min 0.00000 25% 1.09000 50% 1.83000 75% 3.31000 max 5005073.00000 Name: trip_distance, dtype: object trip_distance_percentiles = df[\"trip_distance\"].quantile(np.round(np.arange(0.00, 1.01, 0.01), 2)) trip_distance_percentiles.loc[0.00:0.1] 0.00 0.00 0.01 0.24 0.02 0.38 0.03 0.44 0.04 0.50 0.05 0.53 0.06 0.58 0.07 0.60 0.08 0.64 0.09 0.68 0.10 0.70 Name: trip_distance, dtype: float64 trip_distance_percentiles.loc[0.90:] 0.90 6.48 0.91 7.02 0.92 7.72 0.93 8.52 0.94 9.30 0.95 10.11 0.96 11.09 0.97 12.62 0.98 16.48 0.99 18.31 1.00 5005073.00 Name: trip_distance, dtype: float64 px.histogram(df.sample(500000), x= \"trip_distance\") #obvious outliers (ran several times due to sampling) As the comment above states there are obvious outliers. df.trip_distance.where(lambda x: x \u003e df.trip_distance.min()).min() # Second smallest distance is 0.01 miles or 16 meters 0.01 df.trip_distance.nsmallest(500000).unique() array([0. , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. , 1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09, 1.1 , 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17]) df.trip_distance.nlargest(10) 1059974 5005073.0 1090317 318000.0 1085978 40005.0 1055761 1320.6 1167755 335.9 1125147 302.0 1055917 275.8 1134537 272.9 1166016 238.8 1095309 225.2 Name: trip_distance, dtype: float64 before = df.shape[0] print(f'Rows/Size Before: {before}') invalid_distance_idx = df[(df.trip_distance \u003e 100) | (df.trip_distance == 0)].index.tolist() # Remove distance of 0 and greater than 100 miles https://www.google.com/maps/dir/40.5055432,-74.2381798/41.0191685,-73.7871866/@40.9358817,-74.3487811,8.92z/data=!3m1!5s0x89c2f2be234865c9:0x12533050f3f45b3c!4m2!4m1!3e0 df.drop(invalid_distance_idx, inplace=True) df.reset_index(drop=True, inplace= True) after = df.shape[0] print(f'Rows/Size After: {after}') print(f'Removed: {before - after}') Rows/Size Before: 1768323 Rows/Size After: 1765510 Removed: 2813 ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:5","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Trip Speed Cleaning Helps detect abnormal relationships between distance and time (combining the last 2 cleaning procedures) df.insert(loc=df.columns.tolist().index(\"trip_distance\")+1,column=\"trip_distance_km\",value = np.round(df[\"trip_distance\"] * 1.609344,3)) df.insert(loc=df.columns.tolist().index(\"trip_distance_km\")+1,column=\"trip_speed_mi/hr\",value = np.round((df[\"trip_distance\"]/df[\"trip_time_in_secs\"])*60*60,3)) df[\"trip_speed_mi/hr\"].describe().apply(lambda x: '%.5f' % x) count 1765510.00000 mean 13.79154 std 19.18644 min 0.03000 25% 9.12000 50% 12.28000 75% 16.50000 max 7280.00000 Name: trip_speed_mi/hr, dtype: object speed_percentiles = df[\"trip_speed_mi/hr\"].quantile(np.round(np.arange(0.00, 1.01, 0.01), 2)) speed_percentiles.loc[0:0.1] 0.00 0.030 0.01 2.864 0.02 4.045 0.03 4.665 0.04 5.115 0.05 5.480 0.06 5.800 0.07 6.076 0.08 6.327 0.09 6.560 0.10 6.771 Name: trip_speed_mi/hr, dtype: float64 speed_percentiles.loc[0.90:] # very odd maxmimum speed 0.90 22.439 0.91 23.191 0.92 24.000 0.93 25.000 0.94 26.107 0.95 27.424 0.96 29.023 0.97 31.024 0.98 33.715 0.99 37.835 1.00 7280.000 Name: trip_speed_mi/hr, dtype: float64 df[\"trip_speed_mi/hr\"].nlargest(30) 1455995 7280.000 1440280 7155.000 1150060 7069.091 1150552 6222.857 1143343 5811.429 1053937 5340.000 1374463 5130.000 1157752 4551.429 1173308 4385.455 1041760 4075.714 1459064 3894.545 1433255 3577.500 1458672 3570.000 1438038 3350.769 1058099 3305.455 1149133 3276.000 1374628 3120.000 1339895 3105.000 1439642 3067.200 1345988 2674.286 1374860 2595.000 1151062 2520.000 1094388 2487.273 1076385 2392.941 1094307 2310.000 1148658 2243.077 1150085 2243.077 1405731 2229.231 1344046 2160.000 1078396 2052.000 Name: trip_speed_mi/hr, dtype: float64 speeds = df[\"trip_speed_mi/hr\"].values speeds = np.sort(speeds,axis = None) for i in np.arange(0.0, 1.0, 0.1): print(\"{} percentile is {}\".format(99+i, speeds[int(len(speeds) * (float(99+i) / 100) )])) print(\"100 percentile value is \",speeds[-1]) del speeds 99.0 percentile is 37.835 99.1 percentile is 38.4 99.2 percentile is 39.025 99.3 percentile is 39.729 99.4 percentile is 40.518 99.5 percentile is 41.4 99.6 percentile is 42.45 99.7 percentile is 43.722 99.8 percentile is 45.408 99.9 percentile is 48.027 100 percentile value is 7280.0 before = df.shape[0] print(f'Rows/Size Before: {before}') df = df[(df[\"trip_speed_mi/hr\"] \u003e 0) \u0026 (df[\"trip_speed_mi/hr\"] \u003c= 50)].reset_index(drop=True) # 99.9th percentile is 48.027 after = df.shape[0] print(f'Rows/Size After: {after}') print(f'Removed: {before - after}') Rows/Size Before: 1765510 Rows/Size After: 1764465 Removed: 1045 ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:6","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Trip Total Amount, Fare Amount, Surcharge, Tax, Tolls and Tip Cleaning df[\"total_amount\"].describe().apply(lambda x: '%.5f' % x) count 1764465.00000 mean 14.66869 std 11.83378 min 0.00000 25% 8.00000 50% 11.00000 75% 16.50000 max 563.12000 Name: total_amount, dtype: object df[\"fare_amount\"].describe().apply(lambda x: '%.5f' % x) count 1764465.00000 mean 12.30551 std 9.79532 min 0.00000 25% 6.50000 50% 9.50000 75% 14.00000 max 330.00000 Name: fare_amount, dtype: object df[\"surcharge\"].describe().apply(lambda x: '%.5f' % x) count 1764465.00000 mean 0.27433 std 0.33809 min 0.00000 25% 0.00000 50% 0.00000 75% 0.50000 max 5.83000 Name: surcharge, dtype: object df[\"mta_tax\"].describe().apply(lambda x: '%.5f' % x) count 1764465.00000 mean 0.49890 std 0.02340 min 0.00000 25% 0.50000 50% 0.50000 75% 0.50000 max 0.50000 Name: mta_tax, dtype: object df[\"tip_amount\"].describe().apply(lambda x: '%.5f' % x) count 1764465.00000 mean 1.32689 std 2.16548 min 0.00000 25% 0.00000 50% 1.00000 75% 2.00000 max 558.62000 Name: tip_amount, dtype: object df[\"tolls_amount\"].describe().apply(lambda x: '%.5f' % x) count 1764465.00000 mean 0.26283 std 1.22958 min 0.00000 25% 0.00000 50% 0.00000 75% 0.00000 max 97.13000 Name: tolls_amount, dtype: object total_amount_percentiles = df[\"total_amount\"].quantile(np.round(np.arange(0.00, 1.01, 0.01), 2)) total_amount_percentiles.loc[0:0.1], total_amount_percentiles.loc[0.90:] (0.00 0.00 0.01 4.50 0.02 4.50 0.03 5.00 0.04 5.00 0.05 5.50 0.06 5.50 0.07 5.62 0.08 6.00 0.09 6.00 0.10 6.00 Name: total_amount, dtype: float64, 0.90 26.50 0.91 28.50 0.92 30.50 0.93 33.33 0.94 36.12 0.95 39.33 0.96 43.03 0.97 48.90 0.98 57.83 0.99 65.60 1.00 563.12 Name: total_amount, dtype: float64) df.total_amount.nsmallest(20) 1031853 0.0 1035254 0.0 1041181 0.0 1043898 0.0 1044443 0.0 1047473 0.0 1050744 0.0 1053522 0.0 1055058 0.0 1055089 0.0 1056811 0.0 1060794 0.0 1061535 0.0 1064759 0.0 1071199 0.0 1072267 0.0 1074670 0.0 1076662 0.0 1092351 0.0 1098501 0.0 Name: total_amount, dtype: float64 df.total_amount.nlargest(20) 1122353 563.12 1470048 330.00 1407367 325.00 1429526 300.60 1445864 300.00 685414 298.83 1291224 287.84 1255443 269.00 910977 267.60 74407 266.55 456863 261.75 1114236 260.00 632331 256.63 1555698 255.33 897057 254.50 1721477 254.23 1617697 252.00 1223427 250.63 1239784 250.00 1183020 249.00 Name: total_amount, dtype: float64 totals = df[\"total_amount\"].values totals = np.sort(totals,axis = None) for i in np.arange(0.0, 1.0, 0.1): print(\"{} percentile is {}\".format(99+i, totals[int(len(totals) * (float(99+i) / 100) )])) print(\"100 percentile value is \",totals[-1]) del totals 99.0 percentile is 65.6 99.1 percentile is 67.7 99.2 percentile is 67.93 99.3 percentile is 68.23 99.4 percentile is 68.23 99.5 percentile is 69.38 99.6 percentile is 70.33 99.7 percentile is 72.28 99.8 percentile is 78.0 99.9 percentile is 91.66 100 percentile value is 563.12 before = df.shape[0] print(f'Rows/Size Before: {before}') df = df[(df[\"total_amount\"] \u003e= 0) \u0026 (df[\"total_amount\"] \u003c= 150)].reset_index(drop=True) # See: https://www.introducingnewyork.com/taxis after = df.shape[0] print(f'Rows/Size After: {after}') print(f'Removed: {before - after}') Rows/Size Before: 1764465 Rows/Size After: 1764321 Removed: 144 fare_amount_percentiles = df[\"fare_amount\"].quantile(np.round(np.arange(0.00, 1.01, 0.01), 2)) fare_amount_percentiles.loc[0:0.1], fare_amount_percentiles.loc[0.90:] (0.00 0.0 0.01 3.5 0.02 4.0 0.03 4.0 0.04 4.0 0.05 4.5 0.06 4.5 0.07 4.5 0.08 5.0 0.09 5.0 0.10 5.0 Name: fare_amount, dtype: float64, 0.90 23.0 0.91 24.0 0.92 25.5 0.93 27.5 0.94 29.5 0.95 32.0 0.96 34.5 0.97 40.0 0.98 52.0 0.99 52.0 1.00 150.0 Name: fare_amount, dtype: float64) before = df.shape[0] print(f'Rows/Size Before: {before}') df = df[(df[\"fare_amount\"] \u003e= 0)].reset_index(drop=True) after = df.shape[0] print(f'Rows/Size After: {after}') print(f'Removed: {before - after}') Rows/Size Before: 1764321 Rows/Size After: 1764321 Removed: 0 surcharge_percentiles =","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:7","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Passenger Count Cleaning df[\"passenger_count\"].describe().apply(lambda x: '%.5f' % x) # Looks clean but we have passenger counts of 0 count 1764321.00000 mean 2.02239 std 1.65448 min 0.00000 25% 1.00000 50% 1.00000 75% 2.00000 max 6.00000 Name: passenger_count, dtype: object print(df[df['passenger_count'] == 0].shape[0]) #7 trips with passenger count 0, replace with mean value df.loc[df[df['passenger_count'] == 0].index,'passenger_count'] = df.passenger_count.mean() 7 df.passenger_count.quantile(np.round(np.arange(0.00,1.01, 0.01),2)).loc[0.9:] 0.90 5.0 0.91 5.0 0.92 5.0 0.93 5.0 0.94 6.0 0.95 6.0 0.96 6.0 0.97 6.0 0.98 6.0 0.99 6.0 1.00 6.0 Name: passenger_count, dtype: float64 ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:8","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Categorical Columns Cleaning df.select_dtypes(exclude=[\"number\",\"datetime\"]).columns #rate code is missing as its numerical, but will be treated as categorical Index(['medallion', 'hack_license', 'vendor_id', 'store_and_fwd_flag', 'payment_type'], dtype='object') df.rate_code.unique() # Should only be 1,2,3,4,5,6 from data description, See: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf array([1, 2, 4, 3, 5, 6, 0], dtype=int64) allowed = [1,2,3,4,5,6] abnormal_rate_code = df[~df.rate_code.isin(allowed)] print(abnormal_rate_code.shape[0]) 33 We can use the Folium library to plot maps with markers and lines on it (and much more). We‚Äôll use it to inspect some of these (14) abnormal rate code (rate code = 0) trips newyork_center= folium.Map(location=[40.7345, -73.8697]) # https://www.google.com/search?q=middle+of+new+york+coordinates\u0026rlz=1C1CHBF_enDE1000DE1000\u0026oq=middle+of+new+york+coordina\u0026aqs=chrome.1.69i57j33i22i29i30l6.7294j0j7\u0026sourceid=chrome\u0026ie=UTF-8 for pickup_lat,pick_long, dropoff_lat, dropoff_long, code in zip(abnormal_rate_code[\"pickup_latitude\"], abnormal_rate_code[\"pickup_longitude\"], abnormal_rate_code[\"dropoff_latitude\"], abnormal_rate_code[\"dropoff_longitude\"], abnormal_rate_code[\"rate_code\"][0:14]): folium.CircleMarker([pickup_lat, pick_long],popup=code,tooltip = \"Pickup\",fill_color= \"#d1005b\",color=\"#d1005b\", fill = True,radius = 5).add_to(newyork_center) folium.CircleMarker([dropoff_lat, dropoff_long],popup=code,tooltip = \"Dropoff\",fill_color= \"#616161\",color=\"#616161\", fill = True,radius = 5).add_to(newyork_center) folium.PolyLine([[pickup_lat, pick_long],[dropoff_lat,dropoff_long]]).add_to(newyork_center) newyork_center They look normal trips, there are 33 so I‚Äôll replace them with rate code 1 i.e. standard rate abnormal_rate_code_idx = abnormal_rate_code.index.tolist() df.loc[abnormal_rate_code_idx, 'rate_code'] = 1 df.payment_type.unique() # Looks fine CSH = Cash, DIS = Dispute, CRD = Credit Card, UNK = Unknown, NOC = No charge array(['CSH', 'DIS', 'CRD', 'UNK', 'NOC'], dtype=object) df.store_and_fwd_flag.unique() # Looks fine Y= store and forward trip, N= not a store and forward trip array(['N', 'Y', nan], dtype=object) df.vendor_id.unique() # Looks fine CMT = Creative Mobile Technologies, VTS = VeriFone Inc. array(['CMT', 'VTS'], dtype=object) df.to_csv(\"../Data/Pre-processed Data/cleaned_data.csv\",index=False) They all look as expected so we finally save our clean dataset and onto EDA! ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:4:9","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Data Analysis Exploratory Data Analysis is the process of studying data and extracting insights from it in order to examine its major properties. EDA may be accomplished through the use of statistics and graphical approaches (using my favourite plotting library Plotly! With a bit of matplotlib as well üòÅ). Why is it important? We simply can‚Äôt make sense of such huge datasets if we don‚Äôt explore the data. Exploratory Data Analysis helps us look deeper and see if our intuition matches with the data. It helps us see if we are asking the right questions. Exploring and analyzing the data is important to see how features are distributed or if and how they are related with each other. How they are contributing to the target variable (varable to be predicted/modelled), if there is any or simply analyzing the features without it. This will eventually help us answer the questions we have around the dataset, allow us to formulate new questions and check if we are even asking the right questions. So let‚Äôs start as usual by importing and loading our the cleaned dataset. ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:0","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Import Libraries and Data Loading import warnings warnings.filterwarnings(\"ignore\") import pandas as pd import numpy as np import scipy.stats as ss from statsmodels.distributions.empirical_distribution import ECDF import matplotlib.pyplot as plt import seaborn as sns import datetime import requests import json import os from tqdm.notebook import tqdm from tabulate import tabulate import geopy.distance from sklearn.model_selection import train_test_split from shapely.geometry import Point,Polygon, MultiPoint,MultiPolygon import folium from folium.plugins import HeatMapWithTime from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=\"taxi-analyis\") import geopandas as gpd from geopandas import GeoDataFrame import geoplot import pygeohash as pgh import pyproj from area import area import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots import plotly.offline as pyo pyo.init_notebook_mode(connected=True) from PIL import Image pd.set_option(\"display.max_columns\",None) np.random.seed(42) df = pd.read_csv(\"../Data/Pre-processed Data/cleaned_data.csv\") ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:1","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Convert dates to datetime df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime']) df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime']) # Too many addresses to geocode, Nominatim usage policy doesn't allow bulk geocoding and will timeout (was going to be used in hover) #df[\"pickup_address\"] = df.apply(lambda x: geolocator.reverse(str(x.pickup_latitude) + \",\" + str(x.pickup_longitude),timeout=5)[0],axis=1) #df[\"dropoff_address\"] = df.apply(lambda x: geolocator.reverse(str(x.dropoff_latitude) + \",\" + str(x.dropoff_longitude),timeout=5)[0],axis=1) ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:2","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Splitting data for test and train sets Prevent exploration on test set (data-snooping) df_train, df_test = train_test_split(df, test_size=0.33,random_state=32) df_train.reset_index(drop=True, inplace=True) df_test.reset_index(drop=True, inplace=True) df_train.shape, df_test.shape ((1182095, 24), (582226, 24)) We start by simply plotting a map of a sample (10,000) pickups and dropoffs to get a rough idea if there is a significant difference. def getBB(df, BB): df_BB = df[(df.pickup_longitude \u003e= BB[0]) \u0026 (df.pickup_longitude \u003c= BB[1]) \u0026 (df.pickup_latitude \u003e= BB[2]) \u0026 (df.pickup_latitude \u003c= BB[3]) \u0026 (df.dropoff_longitude \u003e= BB[0]) \u0026 (df.dropoff_longitude \u003c= BB[1]) \u0026 (df.dropoff_latitude \u003e= BB[2]) \u0026 (df.dropoff_latitude \u003c= BB[3])].reset_index(drop=True) return df_BB def drop_pick_map(df, BB, sample_size = 1000, title = '' ,size=5, opacity=0.8): df_plt = df.sample(sample_size) df_plt = getBB(df_plt, BB) data = [go.Scattermapbox( lat= df_plt['pickup_latitude'] , lon= df_plt['pickup_longitude'], text = ['Trip Distance (mi): '+ str(df_plt['trip_distance'][i]) + \"\u003cbr\u003e\" + 'Trip Speed (mi/hr): ' + str(df_plt['trip_speed_mi/hr'][i]) for i in range(df_plt.shape[0])], hovertext=df_plt['trip_time_in_secs'], customdata = df_plt['total_amount'], mode='markers', marker=dict( size= size, color = 'red', opacity = opacity), name ='Pickups', subplot='mapbox', hovertemplate = \"Latitude (¬∞): %{lat} \u003cbr\u003eLongitude (¬∞): %{lon} \u003cbr\u003eTrip Duration (s): %{hovertext} \u003cbr\u003e%{text} \u003cbr\u003eTrip Amount ($): %{customdata}\" ), go.Scattermapbox( lat= df_plt['dropoff_latitude'] , lon= df_plt['dropoff_longitude'], text = ['Trip Distance (mi): '+ str(df_plt['trip_distance'][i]) + \"\u003cbr\u003e\" + 'Trip Speed (mi/hr): ' + str(df_plt['trip_speed_mi/hr'][i]) for i in range(df_plt.shape[0])], hovertext=df_plt['trip_time_in_secs'], customdata = df_plt['total_amount'], mode='markers', marker=dict( size= size, color = 'blue', opacity = opacity), name ='Dropoffs', subplot ='mapbox2', hovertemplate = \"Latitude (¬∞): %{lat} \u003cbr\u003eLongitude (¬∞): %{lon} \u003cbr\u003eTrip Duration (s): %{hovertext} \u003cbr\u003e%{text} \u003cbr\u003eTrip Amount ($): %{customdata}\" )] layout = go.Layout(autosize=False, mapbox= dict(center= dict( lat=40.721319, lon=-73.987130), style = 'open-street-map', zoom = 8, domain={'x': [0, 0.5], 'y': [0, 1]}), mapbox2 = dict(center= dict( lat=40.721319, lon=-73.987130), style = 'open-street-map', zoom=8, domain={'x': [0.51, 1.0], 'y': [0, 1]}), width=1000, height=600, title = title) fig = go.Figure(data=data, layout=layout) fig.show() return #All hires computationaly expensive, can't use interactive plots opting from matplotlib def all_hires(df, BB, figsize=(12, 12), ax=None, c=('r', 'b')): if ax == None: fig, ax = plt.subplots(1, 1, figsize=figsize) df_sub = getBB(df,BB) ax.scatter(df_sub.pickup_longitude, df_sub.pickup_latitude, c=c[0], s=0.1, alpha=0.5) ax.scatter(df_sub.dropoff_longitude, df_sub.dropoff_latitude, c=c[1], s=0.1, alpha=0.5) plt.legend([\"Pickups\",\"Dropoffs\"]) NYBB = (-74.15, -73.7004, 40.5774, 40.9176) drop_pick_map(df_train, NYBB, title = 'Map of Pickups \u0026 Dropoffs',sample_size=10000) We see a slight difference where the pickups are more on the West of New York in the Manhattan area but the dropoffs are more to the East of New York in the Queens and Brooklyn area. We can also see all the pickups and dropoffs in one graphic below all_hires(df_train,NYBB) We‚Äôll segment the EDA sections by question and we‚Äôll start with our first question ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:3","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Q1: How much do taxis earn per trip if they can only drive 30km or 50km per trip, compared to taxis that have no limit? (i.e. what is the average income for trips \u003c30km or \u003c50km compared to the total revenue?) columns=['trip_distance','trip_speed_mi/hr','trip_time_in_secs'] correlation_amount = pd.DataFrame(index=[\"total_amount\"],columns=columns) for i,col in enumerate(columns): cor = df_train.total_amount.corr(df_train[col]) correlation_amount.loc[:,columns[i]] = cor plt.figure(figsize=(25,5)) g = sns.heatmap(correlation_amount,annot=True,fmt='.4f', annot_kws={\"fontsize\":16}) g.set_yticklabels(['Tip Amount'], fontsize = 16) g.set_xticklabels(['Trip Distance','Trip Speed', 'Trip Duration'], fontsize = 16) g.set_yticklabels(['Total Amount'], fontsize = 16) plt.title(\"Correlation Matrix between Total Amount and Numerical Features\", fontdict={'fontsize':20}) plt.plot() Total Amount is highly positively correlated with the trip distance (0.94) and trip duration (0.83) Significant Difference between Trips less than 5km and all trips, no significant difference from the other restrictions, this is mainly due to the fact that \u003c30km and \u003c50km capture most trips We can also see the difference below in the box plots and histograms df_30_less = df_train[(df_train['trip_distance_km'] \u003c= 30)].reset_index(drop=True) df_50_less = df_train[(df_train['trip_distance_km'] \u003c= 50)].reset_index(drop=True) #more severe df_5_less = df_train[(df_train['trip_distance_km'] \u003c= 5)].reset_index(drop=True) print(\"Mean Total Amount for 30 km or less: \", df_30_less.total_amount.mean()) print(\"Mean Total Amount for 50 km or less: \", df_50_less.total_amount.mean()) print(\"Mean Total Amount for 5 km or less: \", df_5_less.total_amount.mean()) print(\"Mean Total Amount for no distance restriction: \", df_train.total_amount.mean()) Mean Total Amount for 30 km or less: 14.223142284432392 Mean Total Amount for 50 km or less: 14.645665338921816 Mean Total Amount for 5 km or less: 9.734024318446 Mean Total Amount for no distance restriction: 14.6549403897318 fig = go.Figure() fig.add_trace(go.Box(y=df_30_less[\"total_amount\"], name= \"30 km or less earnings distribution\")) fig.add_trace(go.Box(y=df_50_less[\"total_amount\"], name= \"50 km or less earnings distribution\")) fig.add_trace(go.Box(y=df_5_less[\"total_amount\"], name= \"5 km or less earnings distribution\")) fig.add_trace(go.Box(y=df_train[\"total_amount\"],name= \"No distance limit earnings distribution\")) fig = fig.update_layout(title = \"T1: How much do taxis earn per trip according to distance restrictions\") fig.write_html(\"../Figures/T1/T1_fig.html\") all_log = np.log(df_train.total_amount.values) km30_log = np.log(df_30_less.total_amount.values) km50_log = np.log(df_50_less.total_amount.values) fig = plt.figure(figsize=(15,8)) plt.title(\"Log Distribution of Total Amount\") plt.hist(all_log[np.isfinite(all_log)],bins=60, label='No restriction', alpha = 0.5) plt.hist(km30_log[np.isfinite(km30_log)],bins=65,label='30 km or less', alpha=0.5) plt.hist(km50_log[np.isfinite(km50_log)],bins=70, label = '50 km or less',alpha=0.5) plt.legend(loc='upper right') plt.show() fig = plt.figure(figsize=(15,8)) plt.title(\"Distribution of Total Amount\") plt.hist(df_train.total_amount.values,bins=60, label='No restriction', alpha = 0.5) plt.hist(df_30_less.total_amount.values,bins=65,label='30 km or less', alpha=0.5) plt.hist(df_50_less.total_amount.values,bins=70, label = '50 km or less',alpha=0.5) plt.legend(loc='upper right') plt.show() Kolmogorov‚ÄìSmirnov Test is used to test for the significance in the difference between the distributions and again same results are observed analytically ks_30vs_all = ss.ks_2samp(df_train['total_amount'].values, df_30_less['total_amount'].values) ks_50vs_all = ss.ks_2samp(df_train['total_amount'].values, df_50_less['total_amount'].values) ks_5vs_all = ss.ks_2samp(df_train['total_amount'].values, df_5_less['total_amount'].values) print(f\"KS-Statistics between 30 km restriction and no restriction: {ks_30vs","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:4","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Q2: Do more taxi trips start at the North or South? By taking a arbitrary north and south cutoff we get these results south = df_train[df_train.pickup_latitude \u003c 40.66240321639479].reset_index(drop=True) north = df_train[df_train.pickup_latitude \u003e 40.803265534217225].reset_index(drop=True) print(\"Number of trips starting in the south: \",south.shape[0]) print(\"Number of trips starting in the north: \",north.shape[0]) Number of trips starting in the south: 21708 Number of trips starting in the north: 19359 data = [go.Scattermapbox( lat= south['pickup_latitude'] , lon= south['pickup_longitude'], text = ['Trip Distance (mi): '+ str(south['trip_distance'][i]) + \"\u003cbr\u003e\" + 'Trip Speed (mi/hr): ' + str(south['trip_speed_mi/hr'][i]) for i in range(south.shape[0])], hovertext=south['trip_time_in_secs'], customdata = south['total_amount'], mode='markers', marker=dict( size= 5, color = 'red', opacity = 0.8), name ='South Pickups', subplot='mapbox', hovertemplate = \"Latitude (¬∞): %{lat} \u003cbr\u003eLongitude (¬∞): %{lon} \u003cbr\u003eTrip Duration (s): %{hovertext} \u003cbr\u003e%{text} \u003cbr\u003eTrip Amount ($): %{customdata}\" ), go.Scattermapbox( lat= north['pickup_latitude'] , lon= north['pickup_longitude'], text = ['Trip Distance (mi): '+ str(north['trip_distance'][i]) + \"\u003cbr\u003e\" + 'Trip Speed (mi/hr): ' + str(north['trip_speed_mi/hr'][i]) for i in range(north.shape[0])], hovertext=north['trip_time_in_secs'], customdata = north['total_amount'], mode='markers', marker=dict( size= 5, color = 'blue', opacity = 0.8), name ='North Pickups', hovertemplate = \"Latitude (¬∞): %{lat} \u003cbr\u003eLongitude (¬∞): %{lon} \u003cbr\u003eTrip Duration (s): %{hovertext} \u003cbr\u003e%{text} \u003cbr\u003eTrip Amount ($): %{customdata}\" )] layout = go.Layout(autosize=False, mapbox= dict(center= dict( lat=40.721319, lon=-73.987130), style = 'open-street-map', zoom = 8, domain={'x': [0, 1], 'y': [0, 1]}), width=1000, height=750, title = 'South vs North Pickups') fig = go.Figure(data=data, layout=layout) fig.show() By taking the center of Manhatten as a cutoff we get a more intutive answer south2 = df_train[df_train.pickup_latitude \u003c 40.78035634806236].reset_index(drop=True) north2 = df_train[df_train.pickup_latitude \u003e 40.78035634806236].reset_index(drop=True) print(\"Number of trips starting in the south: \",south2.shape[0]) print(\"Number of trips starting in the north: \",north2.shape[0]) Number of trips starting in the south: 1062763 Number of trips starting in the north: 119332 data = [go.Scattermapbox( lat= south2.iloc[0:150000]['pickup_latitude'] , lon= south2.iloc[0:150000]['pickup_longitude'], text = ['Trip Distance (mi): '+ str(south2.iloc[0:150000]['trip_distance'][i]) + \"\u003cbr\u003e\" + 'Trip Speed (mi/hr): ' + str(south2.iloc[0:150000]['trip_speed_mi/hr'][i]) for i in range(south2.iloc[0:150000].shape[0])], hovertext=south['trip_time_in_secs'], customdata = south['total_amount'], mode='markers', marker=dict( size= 5, color = 'red', opacity = 0.8), name ='South Pickups', subplot='mapbox', hovertemplate = \"Latitude (¬∞): %{lat} \u003cbr\u003eLongitude (¬∞): %{lon} \u003cbr\u003eTrip Duration (s): %{hovertext} \u003cbr\u003e%{text} \u003cbr\u003eTrip Amount ($): %{customdata}\" ), go.Scattermapbox( lat= north2.iloc[0:50000]['pickup_latitude'] , lon= north2.iloc[0:50000]['pickup_longitude'], text = ['Trip Distance (mi): '+ str(north2.iloc[0:50000]['trip_distance'][i]) + \"\u003cbr\u003e\" + 'Trip Speed (mi/hr): ' + str(north2.iloc[0:50000]['trip_speed_mi/hr'][i]) for i in range(north2.iloc[0:50000].shape[0])], hovertext=north['trip_time_in_secs'], customdata = north['total_amount'], mode='markers', marker=dict( size= 5, color = 'blue', opacity = 0.8), name ='North Pickups', hovertemplate = \"Latitude (¬∞): %{lat} \u003cbr\u003eLongitude (¬∞): %{lon} \u003cbr\u003eTrip Duration (s): %{hovertext} \u003cbr\u003e%{text} \u003cbr\u003eTrip Amount ($): %{customdata}\" )] layout = go.Layout(autosize=False, mapbox= dict(center= dict( lat=40.721319, lon=-73.987130), style = 'open-street-map', zoom = 8, domain={'x': [0, 1], 'y': [0, 1]}), width=1000, height=750, title = 'South vs North Pickups') fig = go.Figure(data=data, layout=layout) fig.","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:5","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Adding date features def get_time_of_day(time): if time \u003e= datetime.time(6, 0, 1) and time \u003c= datetime.time(12, 0, 0): return 'Morning' elif time \u003e= datetime.time(12, 0, 1) and time \u003c= datetime.time(16, 0, 0): return 'Afternon' elif time \u003e= datetime.time(16, 0, 1) and time \u003c= datetime.time(22, 0, 0): return 'Evening' elif time \u003e= datetime.time(22, 0, 1) or time \u003c=datetime.time(1, 0, 0): return 'Night' elif time \u003e= datetime.time(1, 0, 1) or time \u003c=datetime.time(6, 0, 0): return 'Late night' df_train['pickup_day'] = df_train['pickup_datetime'].dt.day_name() df_train['dropoff_day']= df_train['dropoff_datetime'].dt.day_name() df_train['pickup_month'] = df_train['pickup_datetime'].dt.month_name() df_train['dropoff_month'] = df_train['dropoff_datetime'].dt.month_name() df_train['pickup_hour']=df_train['pickup_datetime'].dt.hour df_train['dropoff_hour']=df_train['dropoff_datetime'].dt.hour df_train['pickup_week_year'] = df_train['pickup_datetime'].dt.isocalendar().week df_train['dropoff_week_year'] = df_train['dropoff_datetime'].dt.isocalendar().week df_train['pickup_month_day'] = df_train['pickup_datetime'].dt.day df_train['dropoff_month_day'] = df_train['dropoff_datetime'].dt.day df_train[\"pickup_time_of_day\"] = df_train[\"pickup_datetime\"].dt.time.apply(lambda x: get_time_of_day(x)) df_train[\"dropoff_time_of_day\"] = df_train[\"dropoff_datetime\"].dt.time.apply(lambda x: get_time_of_day(x)) ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:6","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Adding direction feature def get_direction(pickup_coords, dropoff_coords): def get_bearing(pickup_coords, dropoff_coords): lat1 = np.radians(pickup_coords[0]) lat2 = np.radians(dropoff_coords[0]) diffLong = np.radians(dropoff_coords[1] - pickup_coords[1]) x = np.sin(diffLong) * np.cos(lat2) y = np.cos(lat1) * np.sin(lat2) - (np.sin(lat1) * np.cos(lat2) * np.cos(diffLong)) initial_bearing = np.arctan2(x, y) initial_bearing = np.degrees(initial_bearing) compass_bearing = (initial_bearing + 360) % 360 return compass_bearing directions = ['‚Üë N', '‚Üó NE', '‚Üí E', '‚Üò SE', '‚Üì S', '‚Üô SW', '‚Üê W', '‚Üñ NW'] bearing = get_bearing(pickup_coords, dropoff_coords) idx = round(bearing / 45) % 8 return directions[idx] df_train[\"trip_direction\"] = df_train.apply(lambda x: get_direction([x.pickup_latitude,x.pickup_longitude], [x.dropoff_latitude,x.dropoff_longitude]),axis=1) We can get the trip direction in terms of compass bearing using the above formula. ","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:7","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Q3: Traffic Speed Analysis Provide information about the average speed of traffic in New York Analyze of the traffic speed changes in New York throughout the day Analyze of the traffic speed changes in New York throughout the day print(f\"Average overall taxi speed in New York in 2013: {round(df_train['trip_speed_mi/hr'].mean(),2)} miles per hour or {round(df_train['trip_speed_mi/hr'].mean() * 1.609,2)} km per hour\") print(f\"Maximum taxi speed in New York in 2013: {df_train['trip_speed_mi/hr'].max()} miles per hour or {df_train['trip_speed_mi/hr'].max() * 1.609} km per hour\") print(f\"Minimum taxi speed in New York in 2013: {df_train['trip_speed_mi/hr'].min()} miles per hour or {df_train['trip_speed_mi/hr'].min() * 1.609} km per hour\") Average overall taxi speed in New York in 2013: 13.66 miles per hour or 21.99 km per hour Maximum taxi speed in New York in 2013: 50.0 miles per hour or 80.45 km per hour Minimum taxi speed in New York in 2013: 0.03 miles per hour or 0.04827 km per hour fig_speed = px.histogram(df_train, x=\"trip_speed_mi/hr\",nbins = 100,title='Histogram of Trip Speeds') fig_speed = fig_speed.update_layout(xaxis_title = \"Trip Speed\", yaxis_title= 'Count') fig_speed_pdf = px.histogram(df_train, x=\"trip_speed_mi/hr\",nbins = 100,title='PDF of Trip Speeds', histnorm='probability density') fig_speed_pdf = fig_speed_pdf.update_layout(xaxis_title = \"Trip Speed\", yaxis_title= 'Probability Density') # Data skewed to the right, this is usually a result of a lower boundary in a data set # So if the dataset's lower bounds are extremely low relative to the rest of the data, this will cause the data to skew right. fig_speed.show() table =[df_train['trip_speed_mi/hr'].mean(),df_train['trip_speed_mi/hr'].median(),df_train['trip_speed_mi/hr'].mode().values[0]] print(\"Right Skewed Distribution which means mean \u003e median \u003e mode\",end=\"\\n\\n\") print(tabulate([table],headers=[\"Speed Mean\",\"Speed Median\",\"Speed Mode\"])) Right Skewed Distribution which means mean \u003e median \u003e mode Speed Mean Speed Median Speed Mode ------------ -------------- ------------ 13.6649 12.275 12 Trip Speed Per Hour of Day We‚Äôll inspect the trip speed per hour of the day next pispeed_per_hourofday = df_train.groupby(\"pickup_hour\",as_index=False)['trip_speed_mi/hr'].mean() drspeed_per_hourofday = df_train.groupby(\"dropoff_hour\",as_index=False)['trip_speed_mi/hr'].mean() speed_hour = go.Figure() speed_hour.add_trace(go.Scatter(x=pispeed_per_hourofday.pickup_hour, y=pispeed_per_hourofday['trip_speed_mi/hr'], name='Average Pickup Speed per hour',mode='markers+lines')) speed_hour.add_trace(go.Scatter(x=drspeed_per_hourofday.dropoff_hour, y=drspeed_per_hourofday['trip_speed_mi/hr'], name='Average Dropoff Speed per hour',mode='markers+lines')) speed_hour = speed_hour.update_layout(title = \"Average Trip Speed Per Hour of Day\", yaxis_title = \"Average Trip Speed\", xaxis_title =\"Hour of Day\") speed_hour = speed_hour.update_xaxes(type='category') speed_hour.show() We see 5 AM being the fastest hour on average, as expected it is early in the morning and traffic is at its lowest. We also see that the slowest time on average is 9AM. Trip Speed Per Day of Week # Avergae speed distribution per day order_dict = {\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3,\"Thursday\":4,\"Friday\":5,\"Saturday\":6,\"Sunday\":7} pispeed_per_day = df_train.groupby(\"pickup_day\",as_index=False)['trip_speed_mi/hr'].mean() pispeed_per_day.sort_values(by=[\"pickup_day\"],key=lambda x: x.map(order_dict),inplace=True) drspeed_per_day = df_train.groupby(\"dropoff_day\",as_index=False)['trip_speed_mi/hr'].mean() drspeed_per_day.sort_values(by=[\"dropoff_day\"],key=lambda x: x.map(order_dict),inplace=True) speed_day = go.Figure() speed_day.add_trace(go.Scatter(x=pispeed_per_day.pickup_day, y=pispeed_per_day['trip_speed_mi/hr'], name='Average Pickup Speed per day',mode='markers+lines')) speed_day.add_trace(go.Scatter(x=drspeed_per_day.dropoff_day, y=drspeed_per_day['trip_speed_mi/hr'], name='Average Dropoff Speed per day',mode='mar","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:8","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Analysis","Machine Learning","Big Data","Interactive","Data Visualisation"],"content":"Q4: Traffic Volume Analysis Traffic Volume Per Hour of Day traffic_density_pihour = df_train.groupby(\"pickup_hour\",as_index=False).size() traffic_density_drhour = df_train.groupby(\"dropoff_hour\",as_index=False).size() density_hour = go.Figure() density_hour.add_trace(go.Bar(y=traffic_density_pihour.pickup_hour, x=traffic_density_pihour['size'], name='Pickup Density per hour',orientation='h',width=0.3)) density_hour.add_trace(go.Bar(y=traffic_density_drhour.dropoff_hour, x=traffic_density_drhour['size'], name='Dropoff Density per hour',orientation='h',width=0.3)) density_hour = density_hour.update_layout(title = \"Taxi traffic Density Per Hour\", yaxis_title = \"Hour\", xaxis_title =\"Number of trips\",bargap =0.3,bargroupgap = 1,height=800) density_hour = density_hour.update_yaxes(type='category',categoryorder ='array',categoryarray = traffic_density_pihour.pickup_hour.values.tolist()[::-1]) density_hour.show() Mornings and Evening are the busiest times. Traffic Volume Per Day of Week # Average speed distribution per day order_dict = {\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3,\"Thursday\":4,\"Friday\":5,\"Saturday\":6,\"Sunday\":7} traffic_density_piday = df_train.groupby(\"pickup_day\",as_index=False).size() traffic_density_piday.sort_values(by=[\"pickup_day\"],key=lambda x: x.map(order_dict),inplace=True) traffic_density_drday = df_train.groupby(\"dropoff_day\",as_index=False).size() traffic_density_drday.sort_values(by=[\"dropoff_day\"],key=lambda x: x.map(order_dict),inplace=True) density_day = go.Figure() density_day.add_trace(go.Bar(y=traffic_density_piday.pickup_day, x=traffic_density_piday['size'], name='Pickup Density per Day',orientation='h',width=0.3)) density_day.add_trace(go.Bar(y=traffic_density_drday.dropoff_day, x=traffic_density_drday['size'], name='Dropoff Density per Day',orientation='h',width=0.3)) density_day = density_day.update_layout(title = \"Taxi traffic Density Per Day\", yaxis_title = \"Day\", xaxis_title =\"Number of trips\",bargap =0.3,bargroupgap = 1,height=800) density_day = density_day.update_yaxes(type='category') density_day.show() Traffic Volume per both Hour of Day and Day of Week order_dict = {\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3,\"Thursday\":4,\"Friday\":5,\"Saturday\":6,\"Sunday\":7} density_per_dayhour = df_train.groupby([\"pickup_day\",\"pickup_hour\"],as_index=False).size() density_per_dayhour.sort_values(by=[\"pickup_day\",\"pickup_hour\"],key=lambda x: x.map(order_dict),inplace=True) density_dayhour = px.bar(density_per_dayhour, x=\"pickup_hour\",y=\"size\",facet_row=\"pickup_day\",color='pickup_day', barmode='stack',height=1000,facet_row_spacing=0.03) density_dayhour.add_hline(y=density_per_dayhour[density_per_dayhour.pickup_day == 'Monday']['size'].mean(), line_dash=\"dot\", annotation_text=\"Monday Mean Density\",annotation_position=\"bottom right\",row=7,line_color='red') density_dayhour.add_hline(y=density_per_dayhour[density_per_dayhour.pickup_day == 'Monday']['size'].mean(), line_dash=\"dot\", annotation_text=\"Tuesday Mean Density\",annotation_position=\"bottom right\",row=6,line_color='red') density_dayhour.add_hline(y=density_per_dayhour[density_per_dayhour.pickup_day == 'Monday']['size'].mean(), line_dash=\"dot\", annotation_text=\"Wednesday Mean Density\",annotation_position=\"bottom right\",row=5,line_color='red') density_dayhour.add_hline(y=density_per_dayhour[density_per_dayhour.pickup_day == 'Monday']['size'].mean(), line_dash=\"dot\", annotation_text=\"Thursday Mean Density\",annotation_position=\"bottom right\",row=4,line_color='red') density_dayhour.add_hline(y=density_per_dayhour[density_per_dayhour.pickup_day == 'Monday']['size'].mean(), line_dash=\"dot\", annotation_text=\"Friday Mean Density\",annotation_position=\"bottom right\",row=3,line_color='red') density_dayhour.add_hline(y=density_per_dayhour[density_per_dayhour.pickup_day == 'Monday']['size'].mean(), line_dash=\"dot\", annotation_text=\"Saturday Mean Density\",annotation_position=\"bottom right\",row=2,line_color='red') density_dayhour.add_hline(y=density_per_dayhour[density_per_dayhour","date":"2020-10-16","objectID":"/new-york-taxi-analysis/:5:9","tags":["blogging","data science","machine learning","project","python","data analysis"],"title":"Digging Deep Into the New York Taxi Dataset","uri":"/new-york-taxi-analysis/"},{"categories":["Data Preprocessing","Machine Learning","Tutorial"],"content":"Application","date":"2020-08-14","objectID":"/applying-oversampling/","tags":["blogging","data science","machine learning","oversampling","python"],"title":"Applying Over-Sampling Methods to Highly Imbalanced Data","uri":"/applying-oversampling/"},{"categories":["Data Preprocessing","Machine Learning","Tutorial"],"content":" I mentioned various undersampling approaches for dealing with highly imbalanced data in the earlier post ‚ÄúApplying Under-Sampling Methods to Highly Imbalanced Data‚Äù In this article, I present oversampling strategies for dealing with the same problem. By reproducing minority class examples, oversampling raises the weight of the minority class. Although it does not provide information, it introduces the issue of over-fitting, which causes the model to be overly specific. It is possible that while the accuracy for the training set is great, the performance for unseen¬†datasets is poor. ","date":"2020-08-14","objectID":"/applying-oversampling/:0:0","tags":["blogging","data science","machine learning","oversampling","python"],"title":"Applying Over-Sampling Methods to Highly Imbalanced Data","uri":"/applying-oversampling/"},{"categories":["Data Preprocessing","Machine Learning","Tutorial"],"content":"Oversampling Methods Random Oversampling of the minority class Random oversampling just replicates the minority class instances at random. Overfitting is thought to be more likely when random oversampling is used. Random undersampling, on the other hand, has the main disadvantage of discarding useful data. Synthetic Minority Oversampling Technique (SMOTE) Chawla et al. (2002) present the Synthetic Minority Over-sampling Technique to avoid the over-fitting problem (SMOTE). This approach, which is regarded state-of-the-art, is effective in many different applications. Based on feature space similarities between existing minority occurrences, this approach produces synthetic data. To generate a synthetic instance, it locates the K-nearest neighbours of each minority instance, chooses one at random, and then performs linear interpolations to generate a new minority instance in the neighbourhood. ADASYN: Adaptive Synthetic Sampling Motivated by SMOTE, He et al. (2009) introduce and garner widespread attention for the Adaptive Synthetic sampling (ADASYN) approach. ADASYN creates minority class samples based on their density distributions. More synthetic data is generated for minority class samples that are more difficult to learn than for minority class samples that are easier to learn. It computes each minority instance‚Äôs K-nearest neighbours, then uses the class ratio of the minority and majority examples to produce new samples. Repeating this method adaptively adjusts the decision boundary to focus on difficult-to-learn instances. ","date":"2020-08-14","objectID":"/applying-oversampling/:0:1","tags":["blogging","data science","machine learning","oversampling","python"],"title":"Applying Over-Sampling Methods to Highly Imbalanced Data","uri":"/applying-oversampling/"},{"categories":["Data Preprocessing","Machine Learning","Tutorial"],"content":"Application with Python The three oversampling strategies are demonstrated below. The code can be found on my github page. from imblearn.over_sampling import (RandomOverSampler, SMOTE, ADASYN) # RandomOverSampler # With over-sampling methods, the number of samples in a class # should be greater or equal to the original number of samples. sampler = RandomOverSampler(sampling_strategy={1: 2590, 0: 300}) X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'RandomOverSampler {Counter(y_rs)}') plot_data(X_rs,y_rs, 'Random Oversampling') # SMOTE sampler = SMOTE(sampling_strategy={1: 2590, 0: 300}) X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'SMOTE {Counter(y_rs)}') plot_data(X_rs,y_rs,'SMOTE') # ADASYN sampler = ADASYN(sampling_strategy={1: 2590, 0: 300}) X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'ADASYN {Counter(y_rs)}') plot_data(X_rs,y_rs) RandomOverSampler Counter({1: 2590, 0: 300}) SMOTE Counter({1: 2590, 0: 300}) ADASYN Counter({0: 2604, 1: 2589}) The previous article Applying Under-Sampling Methods to Highly Imbalanced Data along with this article together can give you a comprehensive view of both the undersampling and oversampling techniques! ","date":"2020-08-14","objectID":"/applying-oversampling/:0:2","tags":["blogging","data science","machine learning","oversampling","python"],"title":"Applying Over-Sampling Methods to Highly Imbalanced Data","uri":"/applying-oversampling/"},{"categories":["Data Preprocessing","Machine Learning","Tutorial"],"content":"Conclusion I hope this post has helped you better grasp this subject. Thank you very much for reading! üòÑ ","date":"2020-08-14","objectID":"/applying-oversampling/:0:3","tags":["blogging","data science","machine learning","oversampling","python"],"title":"Applying Over-Sampling Methods to Highly Imbalanced Data","uri":"/applying-oversampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"Application","date":"2020-08-12","objectID":"/applying-under-sampling/","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":" Class imbalance can lead to a significant bias toward the dominant class, lowering classification performance and increasing the frequency of false negatives. How can we solve the problem? The most popular strategies include data resampling, which involves either undersampling the majority of the class, oversampling the minority class, or a combination of the two. As a consequence, classification performance will improve. In this article, I will describe what unbalanced data is, why Receiver Operating Characteristic Curve (ROC) fails to measure accurately, and how to address the problem. The second article, Applying Over-Sampling¬†Methods to Highly Imbalanced Data is strongly recommended after or even before this article. The Python code in both posts for anyone who is interested is available in my github.¬†","date":"2020-08-12","objectID":"/applying-under-sampling/:0:0","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"What exactly is imbalanced data? The definition of unbalanced data is simple. A dataset is considered unbalanced if at least one of the classes represents a relatively tiny minority. In finance, insurance, engineering, and many other areas, unbalanced data is common. In fraud detection, it is normal for the imbalance to be on the order of 100 to 1. ","date":"2020-08-12","objectID":"/applying-under-sampling/:0:1","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"Why can‚Äôt a ROC curve measure well? The Receiver operating characteristic (ROC) curve is a common tool for evaluating the performance of machine learning algorithms, however it does not operate well when dealing with unbalanced data. Take an insurance company as an example, which must determine if a claim is legitimate or not. The company must anticipate and avoid potentially fraudulent claims. Assume that 2% of 10,000 claims are fraudulent. The data scientist earns 98% accuracy on the ROC curve if he or she¬†predicts that ALL claims are not fraudulent. However, the data scientist completely overlooked the 2% of actual fraudulent claims. Let‚Äôs frame this decision challenge with either positive or negative labels. A model‚Äôs performance can be expressed by a confusion matrix with four categories. False positives (FP) are negative instances that are wrongly categorised as positives. True positives (TP) are positive examples that are appropriately identified as positives. Similarly, true negatives (TN) are negatives that are appropriately identified as negative, whereas false negatives (FN) are positive instances that are wrongly categorised as negative. Actual positive Actual negative Predicted positive TP FP Predicted negative FN TN We can then define the metrics using the confusion matrix: Name Metric True Positive Rate (TPR) / Recall TP / (TP+FN) False Positive Rate (FPR) FP / (FP+TN) True Negative Rate (TNR) TN / (FP+TN) Precision TP / (TP+FP) The preceding table illustrates that TPR equals TP / P, which only depends on the positive instances. The Receiver Operating Characteristic curve, as illustrated below, plots the TPR against the FPR. The AUC (Area under the curve) measures the overall classification performance. Because AUC does not prioritise one class above another, it does not adequately represent the minority class. Remember that the red dashed line in the figure represents the outcome when there is no model and the data is picked at random. The ROC curve is shown in blue. If the ROC curve is above the red dashed line, the AUC is 0.5 (half of the square area), indicating that the model outcome is no different from a fully random draw. In this research, Davis and Goadrich argue that when dealing with severely imbalanced datasets, Precision-Recall (PR) curves will be more useful than ROC curves. Precision vs. recall is plotted on the PR curves. Because Precision is directly impacted by class imbalance, Precision-recall curves are more effective at highlighting differences across models in highly unbalanced data sets. When comparing various models with unbalanced parameters, the Precision-Recall curve will be more sensitive than the ROC curve. ","date":"2020-08-12","objectID":"/applying-under-sampling/:0:2","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"F1 Score The F1 score should be noted here. It is defined as the harmonic mean of precision and recall as follows: ","date":"2020-08-12","objectID":"/applying-under-sampling/:0:3","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"What are the possible solutions? In general, there are three techniques to dealing with unbalanced data: data sampling, algorithm tweaks, and cost-sensitive learning. This article will focus on data sampling procedures (others maybe in the future). ","date":"2020-08-12","objectID":"/applying-under-sampling/:0:4","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"Undersampling Methods Random Undersampling of the majority class A basic under-sampling strategy is to randomly and evenly under-sample the majority class. This has the potential to result in information loss. However, if the examples of the majority class are close to one another, this strategy may produce decent results. Condensed Nearest Neighbor Rule (CNN) Some heuristic undersampling strategies have been developed to eliminate duplicate instances that should not impair the classification accuracy of the training set in order to prevent losing potentially relevant data. The Condensed Nearest Neighbor Rule (CNN)¬†was proposed by Hart (1968). Hart begins with two empty datasets A and B. The first sample is first placed in dataset A, while the remaining samples are placed in dataset B. Then, using dataset A as the training set, one instance from dataset B is scanned. If a point in B is incorrectly classified, it is moved to A. This process is repeated until there are no points transferred from B to A. TomekLinks Similarly, Tomek (1976) presented an effective strategy for considering samples around the boundary. Given two instances $a$ and $b$ that belong to distinct classes and are separated by a distance $d(a,b)$, the pair $(a, b)$ is termed a Tomek link if there is no instance $c$ such that $d(a,c) \u003c¬†d(a,b)$ or $d(b,c) \u003c¬†d(a,b)$. Tomek link instances are either borderline or noisy, thus both are eliminated. NearMiss The ‚Äúnear neighbour‚Äù strategy and its modifications have been developed to address the issue of potential information loss. The near neighbour family‚Äôs main algorithms are as follows: first, the approach calculates the distances between all instances of the majority class and the instances of the minority class. Then, $k$ examples from the majority class with the shortest distances to those from the minority class are chosen. If the minority class has $n$ instances, the ‚Äúnearest‚Äù method will return $k*n$ instances of the majority class. ‚ÄúNearMiss-1‚Äù picks samples from the majority class whose average distances to the three nearest minority class instances are the shortest. ‚ÄúNearMiss-2‚Äù employs three of the minority class‚Äôs most remote samples. ‚ÄúNearMiss-3‚Äù chooses a predetermined number of closest samples from the majority class for each sample from the minority class. Edited Nearest Neighbor Rule (ENN) Wilson (1972) proposed the Edited Nearest Neighbor Rule (ENN), which requires the removal of any instance whose class label differs from at least two of its three nearest neighbours. The objective behind this strategy is to eliminate examples from the majority class that are near or close to the¬†boundary of distinct classes based on the notion of nearest neighbour (NN) in order to improve the classification accuracy of minority instances rather than majority instances. NeighbourhoodCleaningRule When sampling the data sets, the neighbourhood Cleaning Rule (NCL) treats the majority and minority samples independently. NCL use ENN to eliminate the vast majority of instances. It discovers three nearest neighbours for each instance in the training set. If the instance belongs to the majority class and the classification provided by the instance‚Äôs three nearest neighbours is the opposite of the class of the chosen instance, the instance is deleted. If the chosen instance belongs to the minority class and is misclassified by its three nearest neighbours, the majority class nearest neighbours are eliminated. ClusterCentroids This strategy undersamples the majority class by substituting a cluster of majority samples. Using K-mean techniques, this method discovers the majority class‚Äôs clusters. The cluster centroids of the N clusters are then kept as the new majority samples. ","date":"2020-08-12","objectID":"/applying-under-sampling/:0:5","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"Application with Python The sample strategies are demonstrated below using the python package imbalanced-learn from scikit-learn. The data generation progress (DGP) below creates 2,000 samples with two classes. The data is severely imbalanced, with 0.03 and 0.97 allocated to each class. There are ten features, two of which are informative, two of which are redundant, and six of which are repeated. The make_classification function creates the six repeated (useless) features from the informative and redundant features. The redundant features are simple linear combinations of the informative features. Each class was made up of two gaussian clusters. Informative features are drawn individually from $\\mathcal{N}(0, 1)$ for each cluster and then linearly blended within each cluster. It is critical to understand that if the weights parameter is left blank, the classes are balanced. import pandas as pd import numpy as np import matplotlib.pyplot as plt from collections import Counter from sklearn import datasets from sklearn.decomposition import PCA from sklearn.model_selection import train_test_split from sklearn.metrics import roc_curve,roc_auc_score, precision_recall_curve from xgboost import XGBClassifier from imblearn.datasets import make_imbalance from imblearn.under_sampling import (RandomUnderSampler, ClusterCentroids, TomekLinks, NeighbourhoodCleaningRule,NearMiss) X, y = datasets.make_classification(n_samples = 4000, n_classes = 2, n_clusters_per_class = 2, weights = [0.03,0.97], n_features = 10, n_informative = 2, n_redundant = 2, random_state = 42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) print(f'Original class distribution {Counter(y)}') print(f'Training class distribution {Counter(y_train)}') Original class distribution Counter({1: 3864, 0: 136}) Training class distribution Counter({1: 2589, 0: 91}) I use principal component analysis to reduce the dimensions and select the first two principal components for ease of visualisation. The dataset‚Äôs scatterplot is displayed below. def plot_data(X,y,method): # Use principal component to condense the 10 features to 2 features pca = PCA(n_components=2).fit(X) pca_2d = pca.transform(X) # Assign colors for i in range(0, pca_2d.shape[0]): if y[i] == 0: c1 = plt.scatter(pca_2d[i,0],pca_2d[i,1], c='orange', marker='o') elif y[i] == 1: c2 = plt.scatter(pca_2d[i,0],pca_2d[i,1], c='g', marker='*') plt.legend([c1, c2], ['Class 1', 'Class 2']) plt.title(method) plt.axis([-4, 5, -4, 4]) plt.show() plot_data(X,y,'Original') X_rs, y_rs = make_imbalance(X, y, sampling_strategy={1: 1000, 0: 65}, random_state=0) print(f'Random undersampling {Counter(y_rs)}') plot_data(X_rs,y_rs,'Random undersampling') Random undersampling Counter({1: 1000, 0: 65}) # RandomUnderSampler sampler = RandomUnderSampler(sampling_strategy={1: 1000, 0: 65}) X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'Random undersampling {Counter(y_rs)}') plot_data(X_rs,y_rs,'Random undersampling') # ClusterCentroids sampler = ClusterCentroids(sampling_strategy={1: 1000, 0: 65}) X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'Cluster centriods undersampling {Counter(y_rs)}') plot_data(X_rs,y_rs,'ClusterCentroids') # TomekLinks sampler = TomekLinks() X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'TomekLinks undersampling {Counter(y_rs)}') plot_data(X_rs,y_rs,'TomekLinks') # NeighbourhoodCleaningRule sampler = NeighbourhoodCleaningRule() X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'NearestNeighbours Clearning Rule undersampling {Counter(y_rs)}') plot_data(X_rs,y_rs,'NeighbourhoodCleaningRule') # NearMiss sampler = NearMiss() X_rs, y_rs = sampler.fit_resample(X_train, y_train) print(f'NearMiss{Counter(y_rs)}') plot_data(X_rs,y_rs,'NearMiss') Random undersampling Counter({1: 1000, 0: 65}) Cluster centriods undersampling Counter({1: 1000, 0: 65}) TomekLinks undersampling Counter({1: 2575, 0: 91}) NearestNeighbours Clearning Rule undersamp","date":"2020-08-12","objectID":"/applying-under-sampling/:0:6","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Data Preprocessing","Tutorial","Machine Learning"],"content":"Conclusion I hope this post has helped you better grasp this subject. Thank you very much for reading! üòÑ ","date":"2020-08-12","objectID":"/applying-under-sampling/:0:7","tags":["blogging","data science","machine learning","undersampling","python"],"title":"Applying Under-Sampling Methods to Highly Imbalanced Data","uri":"/applying-under-sampling/"},{"categories":["Tutorial","Machine Learning"],"content":"Details and Application","date":"2020-07-09","objectID":"/logistic-regression/","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Introduction The logistic model (or logit model) in statistics is a statistical model that represents the probability of an event occurring by making the log-odds for the event a linear combination of one or more independent variables. Logistic regression is another approach borrowed from statistics by machine learning. It is the go-to strategy for binary classification problems (problems with two classes) even though it is a regression algorithm (predicts probabilities, more on that later). For example, To predict whether an email is spam (1) or not (0) Whether the tumor is malignant (1) or not (0) This post will discuss the logistic regression algorithm for machine learning. What‚Äôs the Problem with Linear Regression for Classification? The linear regression model is effective for regression but ineffective for classification. Why is this the case? If you have two classes, you may label one with 0 and the other with 1 and use linear regression. It works technically, and most linear model programmes will generate weights for you. However, there are a couple issues with this approach: A linear model does not produce probabilities, but rather treats the classes as integers (0 and 1) and finds the optimum hyperplane (for a single feature, a line) that minimises the distances between the points and the hyperplane. As a result, it just interpolates between the points and cannot be interpreted as probabilities. A linear model will also extrapolate numbers below zero and above one. This is a promising hint that there may be a more intelligent method to classification. Linear models are not applicable to multi-class classification issues. The next class would have to be labelled with 2, then 3, and so on. Although the classes may not be in any meaningful order, the linear model would impose an odd structure on the relationship between the features and your class predictions. The higher the value of a positive-weighted feature, the more it contributes to the prediction of a class with a higher number, even if classes with similar numbers are not closer than other classes. Because the predicted outcome is a linear interpolation between points rather than a probability, there is no meaningful threshold at which one class can be distinguished from the other. A decent example of this problem may be seen on Stackoverflow. So what can be a solution to classification problems, well there are many but one solution is logistic regression, here come the logistic function (or the sigmoid function) ","date":"2020-07-09","objectID":"/logistic-regression/:0:1","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"The Logistic Function Logistic regression is named after the logistic function, which is at the heart of the algorithm. The logistic function, also known as the sigmoid function, was devised by statisticians to characterise the characteristics of rapid population expansion in ecology that exceeds the carrying capacity of the ecosystem (check here). It‚Äôs an S-shaped curve that can transfer any real-valued integer to a value between 0 and 1, but never exactly between those bounds. $$f(z) = \\frac{1}{1 + e^{-z}}$$ Where $e$ is the natural logarithm base (Euler‚Äôs number) and z is the actual numerical value to be transformed. Below we can see the logistic/sigmoid function applied to a range of numbers between -10 and 10 transformed into the range 0 and 1 using the logistic function. Now that we‚Äôve defined the logistic function, let‚Äôs look at how it‚Äôs employed in logistic regression. ","date":"2020-07-09","objectID":"/logistic-regression/:0:2","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Logistic Regression Algorithm Representation Logistic regression, like linear regression, uses an equation as its representation. The transition from linear regression to logistic regression is rather simple. We used a linear equation to model the link between the outcome and the features in the linear regression model: We prefer probabilities between 0 and 1 for classification, so we wrap the right side of the equation in the logistic function. This constrains the output to only accept values between 0 and 1. To predict the output value ($y$), the input values ($X$) are linearly combined using weights or coefficient values (referred to as the Greek capital letter Beta). The coefficients in the equation are the real representation of the model that you would store in memory or in a file. ","date":"2020-07-09","objectID":"/logistic-regression/:0:3","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Why Logistic Regression is Regression not Classification Simply put, logistic regression predicts probabilities which are continous values (i.e., regression).The probability of the default class is modelled using logistic regression (e.g. the first class). For example, if we are modelling people‚Äôs gender based on their height as male or female, the first class may be male, and the logistic regression model could be expressed as the probability of male given a person‚Äôs height, or more formally: In other words, we are modelling the probability that an input (X) belongs to the default class (Y=1); we can express this formally as: Probability Prediction It should be noted that in order to make a probability prediction, the probability prediction must be translated into a binary value (0 or 1). More on this later when we discuss making¬†predictions. Logistic regression is a linear method, however the logistic function is used to alter the predictions. As a result, we can no longer understand the predictions as a linear combination of the inputs, as we can with linear regression. To continue from above, the model can be described as: We can transform the previous equation as follows (remembering that we may eliminate the $e$ (exp) from one side by adding a natural logarithm ($ln$) to the other): This is beneficial because we can see that the output on the right is linear again (exactly like linear regression), and the input on the left is a log of the likelihood of the default class. This ratio on the left is known as the default class‚Äôs odds (it‚Äôs historical that we use odds rather than probabilities; for example, odds are used in boxing rather than probabilities). Odds are calculated as a ratio of the event‚Äôs likelihood divided by the event‚Äôs probability of not occurring. For example, 0.5/(1-0.5) which has the odds of 1. So we could instead write: Because the odds are log transformed, the left hand side is referred to as the log-odds or the probit. Although different types of functions can be used for the transform, the transform that relates the linear regression equation to the probabilities is commonly referred to as the link function, for example, the probit link function. We can reposition the exponent to the right and write it as: All of this helps us understand that the model is still a linear combination of the inputs, but that this linear combination is related to the default class‚Äôs log-odds. ","date":"2020-07-09","objectID":"/logistic-regression/:0:4","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Learning the Logistic Regression Model Parameters The logistic regression algorithm‚Äôs coefficients must be estimated using your training data. This is typically accomplished through the use of maximum-likelihood estimation (gradient-descent can also be used, article on that in the future) Although it does make assumptions about the distribution of your data, maximum-likelihood estimation (MLE) is a frequent learning method utilised by a variety of machine learning algorithms. The best coefficients would result in a model that predicted a value very close to 1 (for example, male) for the default class and a value very close to 0 (for example, female) for the other class. The idea behind maximum-likelihood logistic regression is that a search algorithm seeks coefficient values that minimise the error (see The loss function) between the probabilities predicted by the model and those in the data (e.g. probability of 1 if the data is the primary class). It is sufficient to state that a minimization algorithm is utilised to optimise the coefficient values for the training data. The loss function and MLE So, to learn the coefficients ($\\beta$) of a logistic regression model, we need to define a cost function. MLE is a specific type of probability model estimation, where the loss/objective function is the log likelihood (or minimizing the negative log likelihood) denoted by $\\mathcal{L}$. Suppose we have a set of experimental observations: and a class of distributions $p(x;\\theta)$, where $\\theta$ is a set of parameters on which the form of $p$ depends. The MLE for the distribution from which the dataset has been extracted is defined as the one which maximizes the quantity as $\\theta$ varies. Therefore, in the hypothesis that variables are i.i.d. (Independent and identically distributed) the best value for $\\theta$ will be given by Because multiplication easily overflows/underflows, the equivalent expression is frequently used instead of this one Let‚Äôs look at how this method can be used to construct the log-likelihood loss function (also known as cross-entropy). Surprisingly, while training machine learning algorithms, the maximum likelihood estimation method can be used to create loss functions. In fact, one can imagine working in a scenario in which a dataset with specific statistical qualities is generated, and then construct a loss function that takes those properties into account automatically. Log likelihood or cross-entropy is the mainly used loss function in classification problems, namely problems in which the underlying distribution has a discrete set of output values. Just as an example one can imagine a Bernoulli distribution, which has two outputs, one having probability $p$ and the other $(1‚àíp)$ of being extracted. The negative log likelihood is defined as and in information theory it quantifies the average number of bits needed to identify an event drawn from a set if a coding scheme used for the set is optimized for an estimated probability distribution $q$, rather than the true distribution $p$. This quantity can also be derived using MLE. Suppose data extracted according to a distribution $p(x)$ and an estimated distribution $q(x)$. Let‚Äôs also define the class of estimated probability distributions $\\hat{y} = \\hat{g}(x; \\vec{\\theta})$. Then the optimal parameters are obtained minimizing where $\\Omega$ is the sample space on which the probability space is defined. This way negative log likelihood is recovered as well. ","date":"2020-07-09","objectID":"/logistic-regression/:0:5","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Prediction with Logistic Regression Predictions with a logistic regression model are as easy as plugging numbers into the logistic regression equation and computing the result. Let‚Äôs put this into context with an example. Imagine that we had a model that can determine a person‚Äôs gender based on their height (completely fictitious). Is the person male or female given a height of 192cm? Assuming we now know (learned) the coefficients $\\beta_{0} = -70$ and $\\beta_{1} = 0.4$. Using the aforementioned equation, we can determine the probability of a male given a height of 165cm, or $P(male|height=165)$. Or a very low probability of that the person is a male. In practise, the probabilities can be used immediately. Because this is a classification problem and we want a¬†useful result, we can convert the probabilities to a binary class value, such as ","date":"2020-07-09","objectID":"/logistic-regression/:0:6","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Using Logistic Regression on a dataset I‚Äôll be using the Rain In Australia dataset found in kaggle. The dataset contains information regarding daily weather measurements from a variety of sites in Australia and the aim is to predict whether it will rain the next day or not (Yes or No), which is a binary classification problem that is well suited for logistic regression. I implemented Logistic Regression with Python and Scikit-Learn using its LogisticRegression class. All code can be found in my github. I first import the libraries and load the dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import accuracy_score from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report import category_encoders as ce df = pd.read_csv(\"./data/weatherAUS.csv\") Then I conduct some Exploratory Data Analysis (EDA), some cleaning and data preparation for modelling Dataset shape df.shape (145460, 23) Preview the dataset df.head() Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am ... Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow 0 2008-12-01 Albury 13.4 22.9 0.6 NaN NaN W 44.0 W ... 71.0 22.0 1007.7 1007.1 8.0 NaN 16.9 21.8 No No 1 2008-12-02 Albury 7.4 25.1 0.0 NaN NaN WNW 44.0 NNW ... 44.0 25.0 1010.6 1007.8 NaN NaN 17.2 24.3 No No 2 2008-12-03 Albury 12.9 25.7 0.0 NaN NaN WSW 46.0 W ... 38.0 30.0 1007.6 1008.7 NaN 2.0 21.0 23.2 No No 3 2008-12-04 Albury 9.2 28.0 0.0 NaN NaN NE 24.0 SE ... 45.0 16.0 1017.6 1012.8 NaN NaN 18.1 26.5 No No 4 2008-12-05 Albury 17.5 32.3 1.0 NaN NaN W 41.0 ENE ... 82.0 33.0 1010.8 1006.0 7.0 8.0 17.8 29.7 No No 5 rows √ó 23 columns Get column names for col in df.columns.values.tolist(): print(col+\",\", end=' ') Date, Location, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustDir, WindGustSpeed, WindDir9am, WindDir3pm, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, Temp9am, Temp3pm, RainToday, RainTomorrow View summary of dataset df.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 145460 entries, 0 to 145459 Data columns (total 23 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 145460 non-null object 1 Location 145460 non-null object 2 MinTemp 143975 non-null float64 3 MaxTemp 144199 non-null float64 4 Rainfall 142199 non-null float64 5 Evaporation 82670 non-null float64 6 Sunshine 75625 non-null float64 7 WindGustDir 135134 non-null object 8 WindGustSpeed 135197 non-null float64 9 WindDir9am 134894 non-null object 10 WindDir3pm 141232 non-null object 11 WindSpeed9am 143693 non-null float64 12 WindSpeed3pm 142398 non-null float64 13 Humidity9am 142806 non-null float64 14 Humidity3pm 140953 non-null float64 15 Pressure9am 130395 non-null float64 16 Pressure3pm 130432 non-null float64 17 Cloud9am 89572 non-null float64 18 Cloud3pm 86102 non-null float64 19 Temp9am 143693 non-null float64 20 Temp3pm 141851 non-null float64 21 RainToday 142199 non-null object 22 RainTomorrow 142193 non-null object dtypes: float64(16), object(7) memory usage: 25.5+ MB We can see that the dataset contains mixture of categorical and numerical variables. Categorical variables have data type object. Numerical variables have data type float64. Also, there are some missing values in the dataset. We will explore it later. View statistical properties of dataset df.describe() MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm count 143975.000000 144199.000000 142199.000000 82670.000000 75625.000000 135197.000000 143693.000000 142398.000000 142806.000000 140953.000000 130395.000","date":"2020-07-09","objectID":"/logistic-regression/:0:7","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Multivariate Analysis An important step in EDA is to discover patterns and relationships between variables in the dataset. I will use heat map and pair plot to discover the patterns and relationships in the dataset. First of all, I will draw a heat map. correlation = df.corr() plt.figure(figsize=(16,12)) plt.title('Correlation Heatmap of Rain in Australia Dataset') ax = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white') ax.set_xticklabels(ax.get_xticklabels(), rotation=90) ax.set_yticklabels(ax.get_yticklabels(), rotation=30) plt.show() Interpretation From the above correlation heat map, we can conclude that : MinTemp and MaxTemp variables are highly positively correlated (correlation coefficient = 0.74). MinTemp and Temp3pm variables are also highly positively correlated (correlation coefficient = 0.71). MinTemp and Temp9am variables are strongly positively correlated (correlation coefficient = 0.90). MaxTemp and Temp9am variables are strongly positively correlated (correlation coefficient = 0.89). MaxTemp and Temp3pm variables are also strongly positively correlated (correlation coefficient = 0.98). WindGustSpeed and WindSpeed3pm variables are highly positively correlated (correlation coefficient = 0.69). Pressure9am and Pressure3pm variables are strongly positively correlated (correlation coefficient = 0.96). Temp9am and Temp3pm variables are strongly positively correlated (correlation coefficient = 0.86). Pair Plot First of all, I will define extract the variables which are highly positively correlated. num_var = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'WindGustSpeed', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm'] sns.pairplot(df[num_var], kind='scatter', diag_kind='hist', palette='Rainbow') plt.show() Interpretation I have defined a variable num_var which consists of MinTemp, MaxTemp, Temp9am, Temp3pm, WindGustSpeed, WindSpeed3pm, Pressure9am and Pressure3pm variables. The above pair plot shows relationship between these variables. ","date":"2020-07-09","objectID":"/logistic-regression/:1:0","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Split target from features X = df.drop(['RainTomorrow'], axis=1) y = df['RainTomorrow'] # split X and y into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) ","date":"2020-07-09","objectID":"/logistic-regression/:2:0","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Feature Engineering Feature Engineering is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables. Assumption I assume that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers. I will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting. # impute missing values in X_train and X_test with respective column median in X_train for df1 in [X_train, X_test]: for col in numerical: col_median=X_train[col].median() df1[col].fillna(col_median, inplace=True) # check again missing values in numerical variables in X_train X_train[numerical].isnull().sum() MinTemp 0 MaxTemp 0 Rainfall 0 Evaporation 0 Sunshine 0 WindGustSpeed 0 WindSpeed9am 0 WindSpeed3pm 0 Humidity9am 0 Humidity3pm 0 Pressure9am 0 Pressure3pm 0 Cloud9am 0 Cloud3pm 0 Temp9am 0 Temp3pm 0 Year 0 Month 0 Day 0 dtype: int64 Engineering missing values in categorical variables # impute missing categorical variables with most frequent value for df2 in [X_train, X_test]: df2['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace=True) df2['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace=True) df2['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace=True) df2['RainToday'].fillna(X_train['RainToday'].mode()[0], inplace=True) # check missing values in categorical variables in X_train categorical = X_train.select_dtypes(include=['object']).columns X_train[categorical].isnull().sum() Location 0 WindGustDir 0 WindDir9am 0 WindDir3pm 0 RainToday 0 dtype: int64 Engineering outliers in numerical variables We have seen that the Rainfall, Evaporation, WindSpeed9am and WindSpeed3pm columns contain outliers. I will use top-coding approach to cap maximum values and remove outliers from the above variables. def max_value(df3, variable, top): return np.where(df3[variable]\u003etop, top, df3[variable]) for df3 in [X_train, X_test]: df3['Rainfall'] = max_value(df3, 'Rainfall', 3.2) df3['Evaporation'] = max_value(df3, 'Evaporation', 21.8) df3['WindSpeed9am'] = max_value(df3, 'WindSpeed9am', 55) df3['WindSpeed3pm'] = max_value(df3, 'WindSpeed3pm', 57) Encode categorical variables encoder = ce.BinaryEncoder(cols=['RainToday']) X_train = encoder.fit_transform(X_train) X_test = encoder.transform(X_test) X_train = pd.get_dummies(X_train) X_test = pd.get_dummies(X_test) We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called feature scaling. I will do it as follows cols = X_train.columns scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) X_train = pd.DataFrame(X_train, columns=[cols]) X_test = pd.DataFrame(X_test, columns=[cols]) Now we can finally train the logistic regression model ","date":"2020-07-09","objectID":"/logistic-regression/:3:0","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Modelling # instantiate the model logreg = LogisticRegression(solver='liblinear', random_state=42) # fit the model logreg.fit(X_train, y_train) LogisticRegression(random_state=42, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=42, solver='liblinear') y_pred_test = logreg.predict(X_test) predict_proba method predict_proba method gives the probabilities for the target variable(0 and 1) in this case, in array form. # probability of getting output as 0 - no rain logreg.predict_proba(X_test)[:,0] array([0.15163563, 0.703692 , 0.98254885, ..., 0.98586207, 0.9495434 , 0.95416021]) # probability of getting output as 1 - rain logreg.predict_proba(X_test)[:,1] array([0.84836437, 0.296308 , 0.01745115, ..., 0.01413793, 0.0504566 , 0.04583979]) print(f'Model accuracy score: {accuracy_score(y_test, y_pred_test):0.4f}') Model accuracy score: 0.8455 Compare the train-set and test-set accuracy Now, I will compare the train-set and test-set accuracy to check for overfitting. y_pred_train = logreg.predict(X_train) print(f'Training-set accuracy score: {accuracy_score(y_train, y_pred_train):0.4f}') Training-set accuracy score: 0.8483 The training-set accuracy score is 0.8483 while the test-set accuracy to be 0.8455. These two values are quite comparable. So, there is no question of overfitting. In Logistic Regression, we use default value of C = 1 (C is the inverse of regularization strength, regularization wasn‚Äôt discussed in this article, maybe in another article in the future). It provides good performance with approximately 85% accuracy on both the training and the test set. But the model performance on both the training and test set are very comparable. It is likely the case of underfitting. I will increase C and fit a more flexible model. # fit the Logsitic Regression model with C=100 logreg100 = LogisticRegression(C=100, solver='liblinear', random_state=42) # fit the model logreg100.fit(X_train, y_train) LogisticRegression(C=100, random_state=42, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=100, random_state=42, solver='liblinear') # print the scores on training and test set print(f'Training set score: {logreg100.score(X_train, y_train):.4f}') print(f'Test set score: {logreg100.score(X_test, y_test):.4f}') Training set score: 0.8488 Test set score: 0.8456 We can see that, C=100 results in higher test set accuracy and also a slightly increased training set accuracy. So, we can conclude that a more complex model should perform better. Compare model accuracy with null accuracy So, the model accuracy is 0.8501. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class. So, we should first check the class distribution in the test set. # check class distribution in test set y_test.value_counts() No 22098 Yes 6341 Name: RainTomorrow, dtype: int64 We can see that the occurences of most frequent class is 22098. So, we can calculate null accuracy by dividing 22098 by total number of occurences. # check null accuracy score null_accuracy = (22098/(22098+6341)) print('Null accuracy score: {0:0.4f}'. format(null_accuracy)) Null accuracy score: 0.7770 Interpretation We can see that our model accuracy score is 0.8501 but null accuracy score is 0.7759. So, we can conclude that our Logistic Regression model is doing a very good job in predicting the class labels. Now, based on the above analysis we can conclude that our classification model accuracy is very goo","date":"2020-07-09","objectID":"/logistic-regression/:4:0","tags":["blogging","data science","machine learning","logistic regression","python"],"title":"Logistic Regression: With Application and Analysis on the 'Rain in Australia' Dataset","uri":"/logistic-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"All you need to know","date":"2020-05-15","objectID":"/polynomial-regression/","tags":["blogging","data science","machine learning","polynomial regression","python"],"title":"Polynomial Regression","uri":"/polynomial-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Introduction The data we have frequently cannot be fitted linearly, and we must use a higher degree polynomial (such as a quadratic or cubic one) to be able to fit the data. When the relationship between the dependent ($Y$) and the independent ($X$) variables is curvilinear, as seen in the following figure (generated using numpy based on a quadratic equation), we can utilise a polynomial model. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import plotly.graph_objects as go from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures import operator X = 6 * np.random.rand(200, 1) - 3 y = 0.5 * X**2 + X + 2 + np.random.randn(200, 1) fig = plt.figure(figsize=(10,8)) _ = plt.scatter(X,y,s=10) _ = plt.xlabel(\"$X$\", fontsize=16) _ = plt.ylabel(\"$y$\", rotation=0, fontsize=16) Understanding linear regression and the mathematics underlying it is necessary for this topic. You can read my previous article on Linear Regression if you aren‚Äôt familiar with it. Applying a linear regression model to this dataset first will allow us to gauge how well it will perform. model = LinearRegression() model.fit(X, y) y_pred = model.predict(X) fig = plt.figure(figsize=(10,8)) _ = plt.scatter(X, y, s=10) _ = plt.plot(X, y_pred, color='r') plt.show() The plot of the best fit line is: It is clear that the straight line is incapable to depict the data‚Äôs patterns. This is an example of underfitting (‚ÄúUnderfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate..\"). Computing the R2 score of the linear line gives: from sklearn.metrics import r2_score print(f\"R-Squared of the model is {r2_score(y, y_pred)}\") R-Squared of the model is 0.42763591651827204 Combatting Underfitting The complexity of the model must be increased in order to combat underfitting. ","date":"2020-05-15","objectID":"/polynomial-regression/:0:1","tags":["blogging","data science","machine learning","polynomial regression","python"],"title":"Polynomial Regression","uri":"/polynomial-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Why Polynomial Regression? We can add powers of the original features as additional features to create a higher order equation. A linear model, $\\hat{y} = {\\alpha} + {\\beta}_1x$ can be transformed to $\\hat{y} = {\\alpha} + {\\beta}_1x + {\\beta}_2x^{2}$ Linear Model or Not? Given that the coefficients and weights assigned to the features are still linear, this model is still regarded as linear. $x^{2}$ is still only a feature. But the curve we are trying to fit is quadratic in nature. What we are only doing here is adding powers of each feature as new features (interactions between multiple features can also be added as well depending on the implementation), then simply train a linear model on this extended set of features. This is the essence of Polynomial Regression (details later üòâ) The scikit-Learn PolynomialFeatures class can be used to transform the original features into their higher order terms. Then we can train a linear model with those new generated features. Numpy also offers a polynomial regression implementation using numpy.polyfit (check my github for usage). polynomial_features= PolynomialFeatures(degree=2) x_poly = polynomial_features.fit_transform(X) model = LinearRegression() model.fit(x_poly, y) y_poly_pred = model.predict(x_poly) r2 = r2_score(y,y_poly_pred) sort_axis = operator.itemgetter(0) sorted_zip = sorted(zip(X,y_poly_pred), key=sort_axis) x, y_poly_pred = zip(*sorted_zip) fig = plt.figure(figsize=(10,8)) _ = plt.scatter(X, y, s=10) _ = plt.plot(x, y_poly_pred, color='r') plt.show() Fitting a linear regression model on the transformed features gives the following plot. The figure makes it quite evident that the quadratic curve can fit the data more accurately than the linear line. Calculating the quadratic plot‚Äôs R2 score results in: print(f\"R-Squared of the model is {r2}\") R-Squared of the model is 0.8242378566950601 Which is a great improvement from the previous R2 score When we try to fit a curve with degree 10 to the data, we can observe that it ‚Äúpasses through‚Äù more data points than the quadratic and linear plots. polynomial_features= PolynomialFeatures(degree=10) x_poly = polynomial_features.fit_transform(X) model = LinearRegression() model.fit(x_poly, y) y_poly_pred = model.predict(x_poly) r2 = r2_score(y,y_poly_pred) sort_axis = operator.itemgetter(0) sorted_zip = sorted(zip(X,y_poly_pred), key=sort_axis) x, y_poly_pred = zip(*sorted_zip) fig = plt.figure(figsize=(10,8)) _ = plt.scatter(X, y, s=10) _ = plt.plot(x, y_poly_pred, color='r') plt.show() print(f\"R-Squared of the model is {r2}\") R-Squared of the model is 0.831777739222978 We can observe that increasing the degree to 30 causes the curve to pass through more data points. polynomial_features= PolynomialFeatures(degree=30) x_poly = polynomial_features.fit_transform(X) model = LinearRegression() model.fit(x_poly, y) y_poly_pred30 = model.predict(x_poly) r2 = r2_score(y,y_poly_pred30) sort_axis = operator.itemgetter(0) sorted_zip = sorted(zip(X,y_poly_pred30), key=sort_axis) x, y_poly_pred30 = zip(*sorted_zip) fig = plt.figure(figsize=(10,8)) _ = plt.scatter(X, y, s=10) _ = plt.plot(x, y_poly_pred30, color='r') plt.show() print(f\"R-Squared of the model is {r2}\") R-Squared of the model is 0.7419383093794893 For degree 30, the model also accounts for noise in the data. This is an example of overfitting ( ‚ÄúOverfitting is a concept in data science, which occurs when a statistical model fits exactly against its training data. When this happens, the algorithm unfortunately cannot perform accurately against unseen data, defeating its purpose.‚Äù ). Despite the fact that this model passes through many of the data, it will fail to generalise on unseen data, as observed on the decrease in R2 score. Combatting Overfitting To avoid overfitting, we may either increase the number of training samples so that the algorithm does not learn the noise in the system and can become more generalised (adding more data can potentially be an issue if the data is itsel","date":"2020-05-15","objectID":"/polynomial-regression/:0:2","tags":["blogging","data science","machine learning","polynomial regression","python"],"title":"Polynomial Regression","uri":"/polynomial-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"The Bias vs Variance Trade-Off Bias refers to the error due to the model‚Äôs simplistic assumptions in fitting the data. A high bias means that the model is unable to capture the patterns in the data and this results in underfitting. Variance refers to the error due to the complex model trying to fit the data. High variance means the model passes through most of the data points and it results in overfitting the data. The following figure summarizes this concept: The graph below shows that as model complexity increases, bias decreases and variance increases, and vice versa. A machine learning model should ideally have low variance and bias. However, it is nearly difficult to have both. As a result, a trade-off is made in order to produce a good model that performs well on both train and unseen data. Source: Bias-Variance Tradeoff ","date":"2020-05-15","objectID":"/polynomial-regression/:0:3","tags":["blogging","data science","machine learning","polynomial regression","python"],"title":"Polynomial Regression","uri":"/polynomial-regression/"},{"categories":["Tutorial","Machine Learning"],"content":"Polynomial Fitting Details Rank of a matrix Suppose we are given a $mxn$ matrix $A$ with its columns to be $[ a_1, a_2, a_3 ‚Ä¶‚Ä¶a_n ]$. The column $a_i$ is called linearly dependent if we can write it as a linear combination of other columns i.e. $a_i = w_1a_1 + w_2a_2 + ‚Ä¶‚Ä¶. + w_{i-1}a_{i-1} + w_{i+1}a_{i+1} +‚Ä¶.. + w_{n}a_{n}$ where at least one $w_{i}$ is non-zero. Then we define the rank of the matrix as the number of independent columns in that matrix. $Rank(A)$ = number of independent columns in $A$. However there is another interesting property that the number of linearly independent columns is equal to the number of independent rows in a matrix (proof). Hence $Rank(A) ‚â§ min(m, n)$. A matrix is called full rank if $Rank(A) = min(m, n)$ and is called rank deficient if $Rank(A) \u003c min(m, n)$. Pseudo-Inverse of a matrix An $nxn$ square matrix $A$ has an inverse $A^{-1}$ if and only if $A$ is a full rank matrix. However a rectangular $mxn$ matrix $A$ does not have an inverse. If $A^{T}$ denotes the transpose of matrix $A$ then $A^{T}A$ is a square matrix and Rank of $(A^{T}A)$ = $Rank (A)$ (proof). Therefore if $A$ is a full-rank matrix then the inverse of $A^{T}A$ exists. And $(A^{T}A)^{-1}A^{T}$ is called the pseudo-inverse of $A$. We‚Äôll see soon why it is called so. Details As discussed earlier, in polynomial regression, the original features are converted into polynomial features of required degree $(2,3,..,n)$ and then modeled using a linear model. Suppose we are given $n$ data points $pi = [ x_{i1} ,x_{i2} ,‚Ä¶‚Ä¶, x_{im} ]^{T} , 1 ‚â§ i ‚â§ n$ , and their corresponding values $vi$. Here $m$ denotes the number of features that we are using in our polynomial model. Our goal is to find a nonlinear function $f$ that minimizes the error Hence $f$ is nonlinear over $pi$. So let‚Äôs take an example of a quadratic function i.e. with $n$ data points and 2 features. Then the function would be The objective is to learn the coefficients. Hence we have $n$ points $pi$ and their corresponding values $vi$; we have to minimize For each data point we can write equations as Hence we can form the following matrix equation $$Da = v$$ where However the equation is nonlinear with respect to the data points $pi$, it is linear with respect to the coefficients $a$. So, we can solve for a using the linear least square method. We have $$Da = v$$ multiply $D^{T}$ on both sides $$D^{T}Da = D^{T}v$$ Suppose $D$ has a full rank, that is when the columns in $D$ are linearly independent, then $D^{T}D$ has an inverse.Therefore $$(D^{T}D)^{-1}(D^{T}D)a = (D^{T}D)^{-1}D^{T}v$$ We now have $$a = (D^{T}D)^{-1}D^{T}v$$ Comparing it with $Da = v$, we can see that $(D^{T}D)^{-1}D^{T}$ acts like the inverse of $D$. So it is called the pseudo-inverse of $D$. The above used quadratic polynomial function can be generalised to a polynomial function of order or degree $m$. Thank you for reading and I hope now that you are clear with the working of polynomial regression and the mathematics behind. I have used polynomial regression on the Covid-19 data and wrote an article about it, you can read it here. Also check the github repo for the complete code. ","date":"2020-05-15","objectID":"/polynomial-regression/:0:4","tags":["blogging","data science","machine learning","polynomial regression","python"],"title":"Polynomial Regression","uri":"/polynomial-regression/"},{"categories":["Data Analysis","Interactive","Data Visualisation","Machine Learning"],"content":"A deep dive into the Coronavirus data","date":"2020-03-28","objectID":"/covid19-interactive-analysis/","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Data Analysis","Interactive","Data Visualisation","Machine Learning"],"content":" A new virus has plauged us and caused turmoil in the world. This virus has caused shut-downs, lockdowns and in the worst-case will unfortunately cause deaths. It is our job as responsible citizens to do our best to stop it from further spreading, and what better way than for us data scientists to dive deep into the data (of course while wearing masks and keeping the safe distance). So lets dive in. ","date":"2020-03-28","objectID":"/covid19-interactive-analysis/:0:0","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Data Analysis","Interactive","Data Visualisation","Machine Learning"],"content":"Introduction The SARS-CoV-2 virus causes Coronavirus Disease (COVID-19), an infectious respiratory disease. The majority of those infected with the virus will have mild to moderate respiratory symptoms and will recover without the need for medical attention. Some, on the other hand, will become critically unwell and require medical assistance. Serious sickness is more likely to strike the elderly and those with underlying medical disorders such as cardiovascular disease, diabetes, chronic respiratory disease, or cancer. COVID-19 can make anyone sick and cause them to get very ill or die at any age. But how does the virus spread? Coughing, sneezing, and talking are the most common ways for the virus to spread through small droplets. Although the droplets are not normally airborne, persons who are in close proximity to them may inhale them and become infected. By contacting a contaminated surface and subsequently touching their face, people can become sick. Aerosols that can stay suspended in the air for prolonged periods of time in confined places may also be a source of transmission. It is most contagious in the first three days after symptoms develop, but it can also spread before symptoms appear and from asymptomatic people. Which strongly explains the value of wearing well fitted masks and as further illustrated in the GIF below. GIF Source: Infrared video shows the risks of airborne coronavirus spread | Visual Forensics So if we ask ourselves how can we prevent COVID-19 from spreading, we‚Äôll find these main precautions to take: Keep a safe distance away from somebody coughing or sneezing. When physical separation isn‚Äôt possible, wear a mask. Seek medical help if you have a fever, cough, or difficulty breathing. If you‚Äôre sick, stay at home. Hands should be washed frequently. Use soap and water or an alcohol-based hand rub to clean your hands. Keep your hands away from your eyes, nose, and mouth. When you cough or sneeze, cover your nose and mouth with your bent elbow or a tissue. How is the virus detected? Real-time reverse transcription polymerase chain reaction (rRT-PCR) from a nasopharyngeal swab is the usual method of diagnosis. Although chest CT imaging may be useful for diagnosis in patients with a strong suspicion of infection based on symptoms and risk factors, it is not recommended for routine use¬†screening (Wikipedia), which may present an option to utilize computer vision for example by using convolutional neural networks to detect the virus in CT image scans, we‚Äôll explore this another time in a different article. ","date":"2020-03-28","objectID":"/covid19-interactive-analysis/:1:0","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Data Analysis","Interactive","Data Visualisation","Machine Learning"],"content":"Diving Deep into the data All these previous precautions mentioned are of tremendous importance to stop the virus from further spreading but these won‚Äôt allow us to study more concisely where it spreads, why it spreads in areas more than others and how we can flatten the curve, as they are proactive measures, we are trying to analyze the historical (even though the virus is still fairly young) data through reactive measures of data analysis and machine learning, of course proactive (unsupervised learning) measures can also be deployed. That‚Äôs where data analysis comes into play. I dug deep into the data and all the accompaying code in this article can be found in this Github repository: Covid-19-Analysis. The data is provided by John Hopkins University, it is available in their Github repo. The data is updated daily. I will also be using data from Our World In Data and acaps for further data exploration. Lets start by first importing all required libraries. And setting some default settings import pandas as pd import numpy as np import itertools import os import warnings from itertools import tee warnings.filterwarnings('ignore') import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots import plotly.io as pio import plotly.offline as py import ipywidgets as widgets py.init_notebook_mode() pio.renderers.default = \"notebook\" pd.options.plotting.backend = \"plotly\" pd.options.display.max_rows = 20 from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error from sklearn.model_selection import train_test_split #from countryinfo import CountryInfo #Populations not up to date import pypopulation import pycountry I‚Äôll be using plotly for interactive plotting. ipywidgets for interactive user data filtering and selection. Pandas and numpy for data manipulation. scikit-learn for machine learning and further corresponding libraries for other utilities. We‚Äôll then load the datasets using Pandas straight from the source URL, because in this way everytime the code is ran it will have the most up to date data, rather than a downloaded Excel or CSV file. # Only use the columns we need cols = ['iso_code','continent','location','date' ,'total_cases','new_cases','total_deaths', 'new_deaths','total_cases_per_million', 'new_cases_per_million','total_deaths_per_million', 'new_deaths_per_million','new_tests', 'total_tests','total_tests_per_thousand', 'new_tests_per_thousand','new_tests_smoothed', 'new_tests_smoothed_per_thousand', 'tests_units','stringency_index', 'population','population_density', 'median_age','aged_65_older','aged_70_older', 'gdp_per_capita','extreme_poverty', 'cardiovasc_death_rate','diabetes_prevalence', 'female_smokers','male_smokers', 'handwashing_facilities', 'hospital_beds_per_thousand','life_expectancy'] df_stats = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv',delimiter=',') df_stats = df_stats[cols] df_measures = pd.read_excel('https://www.acaps.org/sites/acaps/files/resources/files/acaps_covid19_government_measures_dataset_0.xlsx', header=0,sheet_name='Dataset') df_confirmed = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv') df_deaths = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv') df_recoveries = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv') Shape for Statistics DataFrame: (167554, 34) Shape for Measures DataFrame: (23923, 18) Shape for Confirmed DataFrame: (284, 783) Shape for Deaths DataFrame: (284, 783) Shape for Recovories DataFrame: (269, 783) After loading the data and printing t","date":"2020-03-28","objectID":"/covid19-interactive-analysis/:2:0","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Tutorial","Machine Learning"],"content":"All you need to know","date":"2020-03-10","objectID":"/linear-regression-all-you-need-to-know/","tags":["blogging","data science","machine learning","linear regression","python"],"title":"Linear Regression","uri":"/linear-regression-all-you-need-to-know/"},{"categories":["Tutorial","Machine Learning"],"content":"What a better way to start my blogging journey, than with one of the most fundamental statistical learning techniques, Linear Regression. A straightforward method for supervised learning - supervised learning is the process of training a model on data where the outcome is known before applying it to data where the outcome is unknown -, learning linear regression also helps to understand the overall process of what supervised learning looks like. Linear regression, in particular, is a powerful tool for predicting a quantitative response. It's been around for a while and is the subject of a slew of textbooks. Linear Regression is part of the Generalized Linear Models family (GLM for short). Despite the fact that it may appear tedious in comparison to some of the more current statistical learning approaches, linear regression remains an effective and extensively used statistical learning method. It also provides as a useful starting point for emerging approaches. Many fancy statistical learning methods can be thought of as extensions or generalizations of linear regression. As a result, the necessity of mastering linear regression before moving on to more advanced learning approaches cannot be emphasized. The response to the question \"Is the variable $X$ associated with a variable $Y$, and if so, what is the relationship, and can we use it to predict Y?\" is perhaps the most prevalent goal in statistics and linear regression tries to answer that, it does this based on linear relationships between the independent ($X$) and dependent ($Y$) variables. Simple linear regression creates a model of the relationship between the magnitudes of two variables‚Äîfor example, as X increases, Y increases as well. Alternatively, when X increases, Y decreases. I've used the term simple here because we are only talking about one variable X, but instead of one variable X, we can of course use multiple predictor variables $ X_{1}...X_{n} $, which is often termed Multiple Linear Regression or just simply Linear Regression. I assure you that I have a multitude of examples and explanations that go through all of the finer points of linear regression. But first, let's go through the fundamental concepts. ","date":"2020-03-10","objectID":"/linear-regression-all-you-need-to-know/:0:0","tags":["blogging","data science","machine learning","linear regression","python"],"title":"Linear Regression","uri":"/linear-regression-all-you-need-to-know/"},{"categories":["Tutorial","Machine Learning"],"content":"The fundamental concepts behind linear regression The first step in linear regression is to fit a line to the data using least squares The second step is to compute R2 Finally, compute the p-value for the computed R squared in the previous step The first concept we are going to tackle is fitting a line to the data, what exactly does that mean and how can we do it? To effectively explain that we are going to need some data. I will be using the fish market data from Kaggle which is available here. The dataset contains sales records for seven common fish species seen in fish markets. Some of the features (columns) in dataset are: Species, Weight, Different Lengths, Height and Width. We‚Äôll be using it to estimate the weight of the fish, as we are trying to explain linear regression we‚Äôll first use only one feature to predict the weight, the height (simple linear regression) We‚Äôll first load the dataset using pandas read_csv: df = pd.read_csv(\"data/Fish.csv\") All code in this article can be found in my Github in this repository Now back to our first concept, fitting a line to the data, what does that mean? Fitting a line to the data To clearly explain this concept let‚Äôs first get only one species of fish, in this example we‚Äôll use Perch: df_perch = df[df.Species == 'Perch'] We‚Äôll then simply create a scatter plot of Weight vs. Height. fig = px.scatter(df_perch, x='Height', y='Weight') fig.show() And then we‚Äôll plot a horizontal line across the data, at the average weight (our target variable). mean_val= df_perch.Weight.mean() fig = fig.add_hline(y = mean_val,row= None, col = None) fig.show() Then we calculate the difference between the actual values, the dots representing the weights in the figure, and the line (the mean value). This can be thought of as the distance between them (if you would like to think geometrically). These distances are also called the residuals. Here are the residuals plotted. for weight,height in zip(df_perch.Weight,df_perch.Height): fig.add_shape(type='line',x0=height,x1=height,y0=weight,y1=mean_val,line=dict(width=1,dash='dash',color='red')) fig.show() Then we'll square each distance (residual) from the line to the data and sum them all up. We square the residuals so that the negative and positive residuals don't cancel each other out. residuals_squared = (df_perch.Weight - mean_val)**2 residuals_squared.sum() We get a value of 6646094.253571428. That‚Äôs a very large number, but we don‚Äôt want that we want it to be as low as possible, and how can we do that? That‚Äôs the next step. Third we rotate and/or shift the line a little bit to be able to decrease this sum of squared distances or residuals to its lowest possible value, that is where the name least squares (or ordinary least squares) comes from. To do that we need to understand what exactly a line is mathematically. You probably remember from linear algebra that a line is just this equation $ y = mx +b $, where $m$ is the slope of the line $b$ is the y-intercept, $x$ is the x-axis value and $y$ is the y-axis value. Well to rotate the line we need to iteratively - or mathematically - adjust the y-intercept ($b$) and the slope ($m$) so that the sum of the squared residuals - the error - is the lowest. In linear regression the equation is usually expressed in a different way like so: $\\hat{y} = {\\alpha} + {\\beta}x$, where $ {\\alpha} $ is the y-intercept (sometimes also written as $ {\\beta}_0 $) and $ {\\beta} $ is the slope. In simple linear regression there is only one independent variable - feature - and therefore only one slope - or coefficient -, but in many cases we have many different features that we would like to use in our equation so for every feature $X_n$ added we add its coefficient $ {\\beta}_n$, to be estimated with the y-intercept. In general, such a relationship may not hold perfectly for the mostly unobserved population of values of the feature and target variables; these unobserved variations from the above equation are referred to as rand","date":"2020-03-10","objectID":"/linear-regression-all-you-need-to-know/:0:1","tags":["blogging","data science","machine learning","linear regression","python"],"title":"Linear Regression","uri":"/linear-regression-all-you-need-to-know/"},{"categories":["General"],"content":"The beginning of sharing my journey publicly","date":"2020-03-04","objectID":"/why-i-started-blogging/","tags":["blogging","data science","machine learning"],"title":"Why I started Blogging","uri":"/why-i-started-blogging/"},{"categories":["General"],"content":"I‚Äôve been studying machine learning, data analysis and all-stuff data for almost a year now. It‚Äôs been an exciting journey, I got to talk to interesting people, work on meaningful projects but most importantly learn, which is really my favourite activity. Plus, my passion for data and being able to use it in worthwhile ways, that will help others grew significantly. I already had an impulse to patterns, numbers, math and reasoning which definitely helped shape my ambition in learning machine learning, data analysis and pursuing a career in data science, not because it was labelled ‚ÄúThe Sexiest Job Of The 21st Century‚Äù by the Harvard Business Review back in 2012. After this small but growing journey, I decided to document it, in the form of blog posts. I worked on several projects so I will be talking about them and sharing them. I tried and still try to understand different algorithms and techniques in my own way, so I will write about them in my own way of thinking. I will also be working on new projects, and I‚Äôll try my best to share them as well. There are other several reasons why I decided to start my own blog, here are some of them: It can help others. Others may benefit from the way I went about several of my own projects and my own way of understanding different concepts. It helps me get feedback on my work. While my blog posts can help others, I am not perfect and I might have gotten something wrong and any feedback from others can help me better myself. It‚Äôs like a portfolio/resume of my work but even better! Because not only do I showcase my skills I can get directly contacted for job offers. I‚Äôve used my previous projects with my previous employers to land several jobs. It helps me learn. Organizing information always aids me in formulating my own thoughts. One of the ways to tell if I understand something is to explain it to someone, and I do that all the time with others, but now just only in blog posts. It can also help me get to know others and even collaborate. I‚Äôve made a summary on my previous work with the help of my projects, and notes. I‚Äôll be using this summary to help me write these blog posts and maybe even enhance/update my previous work, and I advise anyone in the field of data science to try and do the same thing, there‚Äôs really nothing negative that can come from it, it doesn‚Äôt have to be an article a day, or even an article a week, you can do it at your own pace and build it up slowly. I will be writing about: Projects I have worked on Data analysis and business intelligence: from data cleaning, pre-processing, visualization, interactivity, applied statistics and data analysis tools. Machine Learning Algorithms: Regression, Classification, Clustering, etc. Deep Learning: Backpropagation, Activation Functions, Computer Vision, Natural Language Processing, etc. And who knows there might be new topics I would like to write about, only time will tell üòâ. I hope I got your attention, and you are willing to embark with me in my journey. I am super excited to write, code and share all the work I have been working on and will be working on. You can of course subscribe to my feed and get all the latest blog posts as soon as they are uploaded to my website. I also write in Medium here: @arebi.mohamed59. All my other social links such as GitHub and LinkedIn are to be found at the bottom of this page. Thank you for reading and see you on soon in the next article. Stay Safe! ","date":"2020-03-04","objectID":"/why-i-started-blogging/:0:0","tags":["blogging","data science","machine learning"],"title":"Why I started Blogging","uri":"/why-i-started-blogging/"},{"categories":null,"content":" Here are some Projects I have worked on and currently working on: Project Name Description Year Status Covid-19 Interactive Data Analysis In-depth analysis of the Covid-19 data. Findings presented in interactive charts. The charts are deployed on both Binder and Heroku. Blog post that talks about the project can be found here and accompanying GitHub Repo 2020 Done‚úîÔ∏è New York Yellow Taxi Dataset Analysis In-depth analysis of the New York yellow Taxi data. Findings presented in interactive charts. Blog post that talks about the project can be found here and accompanying GitHub Repo 2020 Done‚úîÔ∏è ","date":"2020-03-03","objectID":"/projects/my-projects/:0:0","tags":null,"title":"Projects","uri":"/projects/my-projects/"},{"categories":null,"content":" MOHAMMED AREBI The CV can bee found in PDF format using this link: CV Personal Profile A motivated undergraduate who has excellent technical, leadership and organizational skills studying in a top university, Furtwangen University of Applied Sciences in a bachelor‚Äôs degree in International Business Information Systems and a resourceful and extremely motivated Data Scientist with a proven track record of solving difficult business problems which also has great experience with Programming/Software Development, Data Analytics and Machine Learning, Financial, Education and Manufacturing work environments and businesses. Utilizing my expertise in data science I have been pivotal to supporting businesses and organizations with the tools necessary to meet their goals. I like work on meaningful projects and build valuable applications in the area of machine learning. Actively Building. Always Learning. Professional Experience ","date":"2020-03-03","objectID":"/resume/my-resume/:0:0","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"Furtwangen University of Applied Science Furtwangen, Germany Data Scientist May 2022 to Present Working on the KISS (K√ºnstliche Intelligenz Service und Systeme; Artificial Intelligence Service and Systems) project at Furtwangen University of Applied Sciences Project Link: KISS Webpage Creation of synthetic data for anomaly detection. Various data types/formats synthetically created such as time-series data with anomalies and drift, tabular data, and image data. All data was created using python and subsequent libraries, except synthetic image data which was created with the Blender 3D graphics tool along with its Python API (blenderpy). Parameterization of the synthetic data creation. Various parameters created for the synthetic data creation such as the anomalies scale, percentage, if they are in chunks, one sided, or in clusters. Designing and coding of a convolutional autoencoder for image anomaly detection Implementation of various anomaly detection algorithms such us Isolation Forest, One-Class SVM, Autoencoders, etc. Ongoing‚Ä¶. ","date":"2020-03-03","objectID":"/resume/my-resume/:0:1","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"TDK-Micronas Freiburg, Germany Data Scientist Intern September 2021 to March 2022 Analyzing semi-conductor defect and test data using Python on Jupyter and its subsequent libraries (Pandas, NumPy, Plotly, SciPy, statsmodels, sci-kit learn, Dash, Keras, etc.) to extract insight on defects and production processes using advanced statistical methods and machine learning algorithms. Interactive dashboard creation and maintenance using Jupyter, Plotly and ipython widgets for engineers to visualize and analyze data. Enhancing and updating several data analysis notebooks using several machine learning algorithms. Working with Hadoop, Spark, and SQL to query data for analysis and cleaning ABAP Customization on the SAP system for data transfer from SAP to Hadoop and for data manipulation Collaboration with the R\u0026D department to automate chart extraction to several PowerPoint slides using python-pptx library and ipython widgets for interactivity. Adding offset calculations for several tests using floating point arithmetic. Developing further ETL functions from Hadoop to Python pandas. Internship Certificate Internship Certificate is in German, as the company is based in Germany. If a translation is required please write me. ","date":"2020-03-03","objectID":"/resume/my-resume/:0:2","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"Furtwangen University of Applied Science Furtwangen, Germany Professor Assistant October 2020 to February 2021 Assisting IBS program professors with the hybrid technology Tutoring for IBS students Contact with students online to keep the lecture as smooth as possible, forwarding all inquiries to the professor face-to-face. Organizing events for IBS students. Semester Representative/Speaker (Semester Sprecher) ","date":"2020-03-03","objectID":"/resume/my-resume/:0:3","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"Moodio Ltd Tripoli, Libya Data Analyst Intern January 2019 to July 2019 Data analyst intern, creating dashboards and reports on Power BI. Presentation to the high-level management. Writing SQL statements for data entry and retrieval. Analyzing data with R and Python. ","date":"2020-03-03","objectID":"/resume/my-resume/:0:4","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"Almaaref Private School Tripoli, Libya Data Manager Intern June 2018 to December 2018 Managing the school database on MySQL, running SQL queries on several datasets, creating, maintaining tables, ERM models and data entry and retrieval. ","date":"2020-03-03","objectID":"/resume/my-resume/:0:5","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"Talent Center Tripoli, Libya English Teacher October 2017 to May 2018 Teaching Elementary, Pre-Intermediate and Intermediate English at Talent Center for training and languages. (Oxford English File Curriculum). ","date":"2020-03-03","objectID":"/resume/my-resume/:0:6","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"Sofrajee Tripoli, Libya Delivery Driver July 2017 to August 2017 Delivery guy for Sofrajee (food delivery service) , delivering all kinds of foods and drinks to all places in Tripoli. Handled arrangements with restaurants and customers. Academic Education ","date":"2020-03-03","objectID":"/resume/my-resume/:0:7","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"B.Sc. International Business Information Systems (IBS), Data Science and AI Specialization March 2020 to July 2023 Furtwangen University of Applied Sciences Bachelor of Science planned to be achieved: 07/2023 ","date":"2020-03-03","objectID":"/resume/my-resume/:0:8","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":"Irish Leaving Certificate (Grade: 85%) September 2014 to July 2017 International School Of Martyrs Activities and societies: Student Representatives Board, Teacher- Student Union, Tutoring for Elementary and Primary Students, Volunteering in emergency drills Higher Subjects: Biology, Chemistry, Math, Physics, English, Arabic, Economics Knowledge And Skills Technical skills Programming: Python, SQL, HTML, CSS, Java, JavaScript Database Knowledge: MySQL ML \u0026 Data Science: TensorFlow, PyTorch, Pandas, scikit-learn, Others.. Data Literacy: Data Analysis, Applied Statistics Cloud Technologies: Google Cloud Platform Data Visualization: Plotly, Power BI Social skills Teamwork, Strong communication skills, Teaching skills, Active Listening. Organizational skills Time management, Design Thinking, Analytical Thinking, Problem Solving, Highly Organized, Adaptability, Continuous Learning Orientation. Languages Arabic (Native) English (C2) German (B1) Areas of interest Data Science Data Analytics Data Visualisation MLOps Big Data Artificial Intelligence Blogging Awards And Certificates International School Of Martyrs Tripoli, Libya - ‚ÄúPrincipal‚Äôs List‚Äù for excellent performance Awards AXELOS - ITIL 4 Foundation LearnPowerBI - Microsoft Power BI LinkedIn Learning - Master Python for Data Science Additional Information And Extracurricular Activity Organized bake sales and bazars in my senior year and lead the student/teacher union. Background on HTML and flash. Participated in the Libyan afforestation campaign planting trees in several Libyan cities. Helped organize and coordinate with the school to carry out safety drills. Tutored younger students biology and physics in school for a few weeks. Participated in the world youth day in Libya, helping with preparation and activities during the event. Volunteering in a robotics event in Tripoli, Libya helping explain the main concept to visitors. Conducted a seminar at the Libyan Academy in Janzour with Talent center regarding English language history and culture. Volunteered as a coordinator at the annual event by ‚ÄòI am Tawfik‚Äô to raise money for orphans. Design Thinking and Problem-Solving Workshops with Professor Wolfgang Grather Online courses on Agile Management and Leadership ","date":"2020-03-03","objectID":"/resume/my-resume/:0:9","tags":["Resume"],"title":"Resume","uri":"/resume/my-resume/"},{"categories":null,"content":" I LOVE DATA! From a very early age I‚Äôve been a problem solver. I was that kid who would take part in anything to see how it worked, tried to figure out the problem and find ways to solve it, I was very curious but driven as well. I grew up with this trait and other similar traits one of them being analytical, which shaped my ambitions today in being a data analyst/scientist and anything related to that field. I was able to gain good experience in internships and projects (see Projects Page), where I learned skills like Python, Machine Learning, Statistics and calculus, SQL and Power BI. I am currently studying Data Science and Machine Learning at the Furtwangen University of Applied Sciences I am passionate about innovation, and using data science to help companies and communities thrive. I‚Äôm a data-driven and insightful student who is passionate about growing his skillset and working on meaningful projects. If I‚Äôm not cleaning data, understanding data, or pivoting tables of data, I‚Äôm working out, watching documentaries, playing football, hanging out with friends, and sipping on apple juice (Love that too). Specialties: Data Wrangling/Cleaning, Machine Learning, Python (Pandas, sci-kit learn, Tensorflow, Plotly, etc.), SQL and Microsoft Power BI ","date":"2020-03-02","objectID":"/about/about-me/:0:0","tags":null,"title":"About Me","uri":"/about/about-me/"}]